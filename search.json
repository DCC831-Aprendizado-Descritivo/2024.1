[
  {
    "objectID": "seminario1/artigo2.html",
    "href": "seminario1/artigo2.html",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "",
    "text": "O estudo e a análise de séries temporais desempenham um papel crucial em uma ampla gama de campos, desde o monitoramento do varejo até a detecção de anomalias de segurança. Ao longo do tempo, observações contínuas capturam nuances e padrões que podem revelar insights valiosos sobre o comportamento de sistemas complexos. Neste contexto, a capacidade de agrupar séries temporais com base em padrões similares torna-se fundamental para extrair conhecimento significativo.\n\n\nA motivação por trás dessa abordagem é multifacetada. No setor de varejo, por exemplo, a identificação de tendências locais de compra, como picos de vendas durante períodos festivos, pode ser crucial para otimizar estratégias de marketing e estoque. Da mesma forma, em análises financeiras, compreender as tendências de mercado pode orientar decisões de investimento. Em setores como saúde e biomedicina, a análise de variações sazonais e padrões de sono pode contribuir para o desenvolvimento de tratamentos mais eficazes. Além disso, o planejamento de recursos, a detecção de anomalias de segurança e a análise de dados ambientais e climáticos também se beneficiam significativamente da capacidade de identificar e compreender padrões em séries temporais.\nNeste contexto, este artigo aborda a importância da abordagem de agrupamento de séries temporais, com foco no algoritmo Z-groupings. Exploraremos como esse método oferece uma perspectiva única para a identificação de grupos locais em séries temporais, destacando sua relevância e aplicabilidade em diversas áreas de estudo e prática. Ao compreendermos melhor as nuances e potenciais aplicações do Z-groupings, podemos abrir novas oportunidades para análises mais precisas e insights mais profundos em uma variedade de domínios.\n\n\n\nO algoritmo Z-groupings não possui comparativos diretos. No entanto, existem alguns métodos clássicos que realizam tarefas comparáveis:\n\nK-means: Este método particiona séries temporais em k clusters, onde os clusters representam grupos locais análogos aos encontrados pelo Z-groupings.\nAgrupamento Hierárquico: Neste método, as séries temporais são hierarquicamente divididas com base em uma métrica de similaridade. Os grupos resultantes são comparáveis aos grupos locais identificados pelo Z-groupings.\n\nContudo, é importante ressaltar que esses algoritmos não são diretamente comparáveis, pois se limitam a encontrar similaridades dentro de uma única série temporal, não considerando relações entre diferentes séries.\nJá em relação aos algoritmos de mineração de sequências, fica evidente que ambos compartilham diversas características fundamentais. Ambos os métodos têm a capacidade de identificar padrões sequenciais em conjuntos de dados, baseando-se na frequência de ocorrência desses padrões. Além disso, ambos utilizam o conceito de suporte para filtrar padrões menos frequentes, priorizando aqueles que são mais relevantes para a análise.\nEntretanto, ao analisar as diferenças entre o Z-groupings e seus equivalentes na mineração de sequência, destacam-se aspectos distintivos que delineiam a aplicação específica do Z-groupings em contextos de séries temporais. Enquanto muitos algoritmos de mineração de sequência são aplicáveis a diversos tipos de dados, o Z-groupings é especialmente projetado para lidar com séries temporais. Sua funcionalidade principal reside na capacidade de agrupar sequências temporais em grupos locais, visando identificar associações significativas entre os padrões temporais presentes nos dados. Essa abordagem mais focalizada confere ao Z-groupings uma vantagem significativa em cenários onde a compreensão das relações temporais é crucial para a análise e interpretação dos dados.\nEssas nuances ressaltam a importância do Z-groupings como uma ferramenta especializada e eficaz para a análise de séries temporais, oferecendo insights valiosos e facilitando a descoberta de padrões e associações relevantes nos dados.\n\n\n\nSerão examinados três casos específicos que destacam a versatilidade e utilidade do Z-groupings: o agrupamento de séries temporais de consumo de energia elétrica em residências, a análise da relação entre manejos madeireiros e desmatamento usando agrupamento de séries temporais, e a análise de consumo de medicamentos para otimização da logística de distribuição. Cada uma dessas aplicações oferece uma perspectiva única sobre como o Z-groupings pode ser empregado para abordar desafios complexos e promover impactos significativos em diversos setores.\n\n\nO agrupamento de séries temporais de consumo de energia elétrica em residências é uma aplicação-chave das redes inteligentes, impulsionadas pela convergência de sistemas computacionais, de medição e de comunicação. Os medidores inteligentes, responsáveis por capturar e transmitir dados de consumo em intervalos regulares, geram uma quantidade substancial de informações. A análise desses dados, conhecida como agrupamento de curvas de carga, é essencial para extrair insights relevantes. Neste contexto, o Z-groupings e outros algoritmos semelhantes emergem como ferramentas valiosas.\nAs implicações dessa aplicação são diversas:\n\nPrevisão de demanda de energia: Identificar padrões de consumo semelhantes entre diferentes regiões possibilita prever com mais precisão a demanda futura de energia. Isso facilita o planejamento da produção e distribuição de eletricidade pelas empresas de energia.\nDetecção de anomalias: Ao conhecer os padrões de consumo típicos, torna-se mais fácil detectar anomalias que possam indicar falhas nos equipamentos, problemas de eficiência energética ou atividades suspeitas, como roubo de energia.\nPotencial para discriminação: O uso das informações sobre padrões de consumo para segmentar clientes ou estabelecer tarifas diferenciadas pode levar à discriminação. Alguns grupos demográficos podem ser penalizados ou excluídos, aumentando as desigualdades.\nRisco de monopólio: Empresas de distribuição de energia com acesso a recursos computacionais avançados para análise de dados têm uma vantagem competitiva significativa. Isso pode resultar em um desequilíbrio de mercado, com grandes empresas dominando e marginalizando empresas menores.\n\nEssas consequências ressaltam a importância não apenas da aplicação eficaz de algoritmos de agrupamento de séries temporais, como o Z-groupings, mas também da consideração cuidadosa dos impactos sociais e éticos das decisões baseadas em dados no setor de energia elétrica.\n\n\n\nA análise da relação entre manejos madeireiros e desmatamento na Amazônia, por meio do agrupamento de séries temporais, destaca-se como uma aplicação vital dessa técnica analítica. Ao examinar as mudanças no índice de cobertura vegetal ao longo do tempo, é possível identificar padrões e tendências cruciais para o monitoramento e prevenção do desmatamento, bem como para o planejamento estratégico de políticas de conservação.\nAs consequências potenciais desse enfoque são variadas:\n\nMonitoramento e prevenção eficazes do desmatamento: O uso do agrupamento de séries temporais pode melhorar substancialmente as estratégias de prevenção do desmatamento, identificando áreas em risco e facilitando intervenções preventivas.\nPlanejamento de políticas de conservação: A identificação de áreas com alto risco de desmatamento possibilita o direcionamento eficiente de recursos e esforços para medidas preventivas, como programas de educação ambiental e reforço da fiscalização.\nRiscos à privacidade: O monitoramento detalhado pode levantar preocupações de privacidade, revelando informações sensíveis sobre padrões de vida e comportamentos das comunidades locais.\nDesigualdade e discriminação: Restrições ao uso da terra e acesso limitado às ferramentas de análise podem resultar em desigualdades na distribuição de recursos para a conservação.\nImpacto nos meios de subsistência locais: As políticas de conservação devem considerar os impactos nas comunidades locais, garantindo que sejam justas e equitativas.\n\nEssas considerações destacam a importância de uma abordagem ética e abrangente no uso do agrupamento de séries temporais para a análise da relação entre manejos madeireiros e desmatamento na Amazônia.\n\n\n\nA aplicação do método Z-Grouping na análise do consumo e distribuição de medicamentos apresenta uma oportunidade significativa para aprimorar a gestão de recursos farmacêuticos, especialmente sob a perspectiva da saúde pública, com foco na atuação da Agência Nacional de Vigilância Sanitária (ANVISA). Ao identificar padrões temporais e regionais no uso de medicamentos, essa abordagem oferece insights valiosos para intervenções estratégicas na logística de distribuição.\nAs aplicações práticas dessa análise são amplas:\n\nRefinamento da Logística de Distribuição: A identificação de agrupamentos locais de consumo permite ajustes precisos na cadeia de suprimentos, garantindo a disponibilidade adequada de medicamentos essenciais nas regiões com maior demanda, o que é crucial para garantir a continuidade dos tratamentos, especialmente em contextos de doenças crônicas ou surtos de doenças infecciosas.\nProjeção de Demandas Futuras: O Z-Grouping possibilita a projeção de demandas com base em tendências históricas, permitindo a antecipação de necessidades, como vacinas ou medicamentos antivirais. Essa capacidade preditiva é fundamental para evitar escassez ou excesso de estoques, possibilitando uma resposta eficiente às flutuações do mercado e demandas emergentes.\nDiagnóstico de Desperdícios e Ineficiências: Além disso, essa abordagem revela padrões de subutilização ou desperdício de medicamentos, indicando áreas potenciais para otimização das políticas de distribuição e uso racional de recursos farmacêuticos.\n\nEntretanto, a implementação dessas análises não está isenta de desafios e consequências, como:\n\nIntegridade e Segurança dos Dados: A gestão cuidadosa da integridade e segurança dos dados é essencial para prevenir riscos de comprometimento das análises, garantindo o cumprimento das normativas de proteção de dados, como a LGPD.\nJustiça na Distribuição de Medicamentos: Há o risco de que análises baseadas em dados não reflitam com precisão a distribuição demográfica, resultando em alocações de recursos que perpetuam desequilíbrios existentes. Portanto, é crucial que as decisões de política farmacêutica considerem profundamente as necessidades locais.\nRiscos da Dependência de Modelos Quantitativos: Apesar da robustez do Z-Grouping, sua aplicação deve ser complementada por análises qualitativas e conhecimento especializado em saúde pública para evitar decisões que não considerem a complexidade dos padrões de saúde específicos.\n\nEssas considerações destacam a importância de uma abordagem holística e cuidadosa na utilização do Z-Grouping para análise de consumo de medicamentos e sua distribuição, visando garantir benefícios significativos sem comprometer a integridade dos dados ou perpetuar desigualdades existentes."
  },
  {
    "objectID": "seminario1/artigo2.html#contextualização-do-problema",
    "href": "seminario1/artigo2.html#contextualização-do-problema",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "",
    "text": "A motivação por trás dessa abordagem é multifacetada. No setor de varejo, por exemplo, a identificação de tendências locais de compra, como picos de vendas durante períodos festivos, pode ser crucial para otimizar estratégias de marketing e estoque. Da mesma forma, em análises financeiras, compreender as tendências de mercado pode orientar decisões de investimento. Em setores como saúde e biomedicina, a análise de variações sazonais e padrões de sono pode contribuir para o desenvolvimento de tratamentos mais eficazes. Além disso, o planejamento de recursos, a detecção de anomalias de segurança e a análise de dados ambientais e climáticos também se beneficiam significativamente da capacidade de identificar e compreender padrões em séries temporais.\nNeste contexto, este artigo aborda a importância da abordagem de agrupamento de séries temporais, com foco no algoritmo Z-groupings. Exploraremos como esse método oferece uma perspectiva única para a identificação de grupos locais em séries temporais, destacando sua relevância e aplicabilidade em diversas áreas de estudo e prática. Ao compreendermos melhor as nuances e potenciais aplicações do Z-groupings, podemos abrir novas oportunidades para análises mais precisas e insights mais profundos em uma variedade de domínios."
  },
  {
    "objectID": "seminario1/artigo2.html#relacionamento-com-métodos-clássicos",
    "href": "seminario1/artigo2.html#relacionamento-com-métodos-clássicos",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "",
    "text": "O algoritmo Z-groupings não possui comparativos diretos. No entanto, existem alguns métodos clássicos que realizam tarefas comparáveis:\n\nK-means: Este método particiona séries temporais em k clusters, onde os clusters representam grupos locais análogos aos encontrados pelo Z-groupings.\nAgrupamento Hierárquico: Neste método, as séries temporais são hierarquicamente divididas com base em uma métrica de similaridade. Os grupos resultantes são comparáveis aos grupos locais identificados pelo Z-groupings.\n\nContudo, é importante ressaltar que esses algoritmos não são diretamente comparáveis, pois se limitam a encontrar similaridades dentro de uma única série temporal, não considerando relações entre diferentes séries.\nJá em relação aos algoritmos de mineração de sequências, fica evidente que ambos compartilham diversas características fundamentais. Ambos os métodos têm a capacidade de identificar padrões sequenciais em conjuntos de dados, baseando-se na frequência de ocorrência desses padrões. Além disso, ambos utilizam o conceito de suporte para filtrar padrões menos frequentes, priorizando aqueles que são mais relevantes para a análise.\nEntretanto, ao analisar as diferenças entre o Z-groupings e seus equivalentes na mineração de sequência, destacam-se aspectos distintivos que delineiam a aplicação específica do Z-groupings em contextos de séries temporais. Enquanto muitos algoritmos de mineração de sequência são aplicáveis a diversos tipos de dados, o Z-groupings é especialmente projetado para lidar com séries temporais. Sua funcionalidade principal reside na capacidade de agrupar sequências temporais em grupos locais, visando identificar associações significativas entre os padrões temporais presentes nos dados. Essa abordagem mais focalizada confere ao Z-groupings uma vantagem significativa em cenários onde a compreensão das relações temporais é crucial para a análise e interpretação dos dados.\nEssas nuances ressaltam a importância do Z-groupings como uma ferramenta especializada e eficaz para a análise de séries temporais, oferecendo insights valiosos e facilitando a descoberta de padrões e associações relevantes nos dados."
  },
  {
    "objectID": "seminario1/artigo2.html#impacto-social-e-potenciais-aplicações",
    "href": "seminario1/artigo2.html#impacto-social-e-potenciais-aplicações",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "",
    "text": "Serão examinados três casos específicos que destacam a versatilidade e utilidade do Z-groupings: o agrupamento de séries temporais de consumo de energia elétrica em residências, a análise da relação entre manejos madeireiros e desmatamento usando agrupamento de séries temporais, e a análise de consumo de medicamentos para otimização da logística de distribuição. Cada uma dessas aplicações oferece uma perspectiva única sobre como o Z-groupings pode ser empregado para abordar desafios complexos e promover impactos significativos em diversos setores.\n\n\nO agrupamento de séries temporais de consumo de energia elétrica em residências é uma aplicação-chave das redes inteligentes, impulsionadas pela convergência de sistemas computacionais, de medição e de comunicação. Os medidores inteligentes, responsáveis por capturar e transmitir dados de consumo em intervalos regulares, geram uma quantidade substancial de informações. A análise desses dados, conhecida como agrupamento de curvas de carga, é essencial para extrair insights relevantes. Neste contexto, o Z-groupings e outros algoritmos semelhantes emergem como ferramentas valiosas.\nAs implicações dessa aplicação são diversas:\n\nPrevisão de demanda de energia: Identificar padrões de consumo semelhantes entre diferentes regiões possibilita prever com mais precisão a demanda futura de energia. Isso facilita o planejamento da produção e distribuição de eletricidade pelas empresas de energia.\nDetecção de anomalias: Ao conhecer os padrões de consumo típicos, torna-se mais fácil detectar anomalias que possam indicar falhas nos equipamentos, problemas de eficiência energética ou atividades suspeitas, como roubo de energia.\nPotencial para discriminação: O uso das informações sobre padrões de consumo para segmentar clientes ou estabelecer tarifas diferenciadas pode levar à discriminação. Alguns grupos demográficos podem ser penalizados ou excluídos, aumentando as desigualdades.\nRisco de monopólio: Empresas de distribuição de energia com acesso a recursos computacionais avançados para análise de dados têm uma vantagem competitiva significativa. Isso pode resultar em um desequilíbrio de mercado, com grandes empresas dominando e marginalizando empresas menores.\n\nEssas consequências ressaltam a importância não apenas da aplicação eficaz de algoritmos de agrupamento de séries temporais, como o Z-groupings, mas também da consideração cuidadosa dos impactos sociais e éticos das decisões baseadas em dados no setor de energia elétrica.\n\n\n\nA análise da relação entre manejos madeireiros e desmatamento na Amazônia, por meio do agrupamento de séries temporais, destaca-se como uma aplicação vital dessa técnica analítica. Ao examinar as mudanças no índice de cobertura vegetal ao longo do tempo, é possível identificar padrões e tendências cruciais para o monitoramento e prevenção do desmatamento, bem como para o planejamento estratégico de políticas de conservação.\nAs consequências potenciais desse enfoque são variadas:\n\nMonitoramento e prevenção eficazes do desmatamento: O uso do agrupamento de séries temporais pode melhorar substancialmente as estratégias de prevenção do desmatamento, identificando áreas em risco e facilitando intervenções preventivas.\nPlanejamento de políticas de conservação: A identificação de áreas com alto risco de desmatamento possibilita o direcionamento eficiente de recursos e esforços para medidas preventivas, como programas de educação ambiental e reforço da fiscalização.\nRiscos à privacidade: O monitoramento detalhado pode levantar preocupações de privacidade, revelando informações sensíveis sobre padrões de vida e comportamentos das comunidades locais.\nDesigualdade e discriminação: Restrições ao uso da terra e acesso limitado às ferramentas de análise podem resultar em desigualdades na distribuição de recursos para a conservação.\nImpacto nos meios de subsistência locais: As políticas de conservação devem considerar os impactos nas comunidades locais, garantindo que sejam justas e equitativas.\n\nEssas considerações destacam a importância de uma abordagem ética e abrangente no uso do agrupamento de séries temporais para a análise da relação entre manejos madeireiros e desmatamento na Amazônia.\n\n\n\nA aplicação do método Z-Grouping na análise do consumo e distribuição de medicamentos apresenta uma oportunidade significativa para aprimorar a gestão de recursos farmacêuticos, especialmente sob a perspectiva da saúde pública, com foco na atuação da Agência Nacional de Vigilância Sanitária (ANVISA). Ao identificar padrões temporais e regionais no uso de medicamentos, essa abordagem oferece insights valiosos para intervenções estratégicas na logística de distribuição.\nAs aplicações práticas dessa análise são amplas:\n\nRefinamento da Logística de Distribuição: A identificação de agrupamentos locais de consumo permite ajustes precisos na cadeia de suprimentos, garantindo a disponibilidade adequada de medicamentos essenciais nas regiões com maior demanda, o que é crucial para garantir a continuidade dos tratamentos, especialmente em contextos de doenças crônicas ou surtos de doenças infecciosas.\nProjeção de Demandas Futuras: O Z-Grouping possibilita a projeção de demandas com base em tendências históricas, permitindo a antecipação de necessidades, como vacinas ou medicamentos antivirais. Essa capacidade preditiva é fundamental para evitar escassez ou excesso de estoques, possibilitando uma resposta eficiente às flutuações do mercado e demandas emergentes.\nDiagnóstico de Desperdícios e Ineficiências: Além disso, essa abordagem revela padrões de subutilização ou desperdício de medicamentos, indicando áreas potenciais para otimização das políticas de distribuição e uso racional de recursos farmacêuticos.\n\nEntretanto, a implementação dessas análises não está isenta de desafios e consequências, como:\n\nIntegridade e Segurança dos Dados: A gestão cuidadosa da integridade e segurança dos dados é essencial para prevenir riscos de comprometimento das análises, garantindo o cumprimento das normativas de proteção de dados, como a LGPD.\nJustiça na Distribuição de Medicamentos: Há o risco de que análises baseadas em dados não reflitam com precisão a distribuição demográfica, resultando em alocações de recursos que perpetuam desequilíbrios existentes. Portanto, é crucial que as decisões de política farmacêutica considerem profundamente as necessidades locais.\nRiscos da Dependência de Modelos Quantitativos: Apesar da robustez do Z-Grouping, sua aplicação deve ser complementada por análises qualitativas e conhecimento especializado em saúde pública para evitar decisões que não considerem a complexidade dos padrões de saúde específicos.\n\nEssas considerações destacam a importância de uma abordagem holística e cuidadosa na utilização do Z-Grouping para análise de consumo de medicamentos e sua distribuição, visando garantir benefícios significativos sem comprometer a integridade dos dados ou perpetuar desigualdades existentes."
  },
  {
    "objectID": "seminario1/artigo2.html#conceitos-chave",
    "href": "seminario1/artigo2.html#conceitos-chave",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "2.1 Conceitos Chave",
    "text": "2.1 Conceitos Chave\nPara compreender o algoritmo Z-Grouping, é fundamental dominar alguns conceitos fundamentais.\n\n\nCode\nfrom zgrouping.zgrouping import grouping, syntheticGenerator, utils\nimport matplotlib.pyplot as plt\n\n\n\nSéries Temporais: Uma série temporal consiste em observações coletadas sequencialmente ao longo do tempo. Cada observação está vinculada a um instante específico, sendo a ordem das observações de crucial importância. Exemplo: Uma série temporal pode registrar as vendas diárias de um produto ao longo de um período, como as vendas diárias de um modelo de smartphone em uma loja.\n\n\n\nCode\ntc = 50\ntl = 365\nc = 20\nno_outliers = 10\noutlier_size = 10\n\n# GROUPING GENERATION\nn_bins = 5\nalpha = 0.9\neta = 1.5\n\nX_raw, y = syntheticGenerator.createSyntheticData(tc = tc, tl=tl, c = c, no_outliers = no_outliers, outlier_size=outlier_size)\nplt.plot(X_raw[0])\n\n\n\n\n\n\n\n\n\n\nAbstração Temporal: A abstração temporal é o processo de simplificar ou extrair características mais significativas de uma série temporal, facilitando sua análise. Exemplo: A aplicação do Symbolic Aggregate Approximation (SAX) para converter uma série temporal de vendas diárias em uma sequência de símbolos que representam padrões de vendas ao longo do tempo.\nEventos em Séries Temporais: Um evento em uma série temporal é uma ocorrência distinta ou uma característica identificável nos dados ao longo do tempo, como picos, vales, transições ou padrões recorrentes.\nRótulos de Eventos: Os rótulos de eventos são atributos simbólicos ou categorizações aplicadas aos eventos em uma série temporal para representá-los de maneira simplificada e compreensível. Exemplo: Os rótulos podem ser escolhidos de um conjunto discreto de símbolos ou categorias, como letras, números ou outros identificadores simbólicos.\nMatriz de Sequência de Eventos: Uma matriz que representa a sequência de rótulos de eventos derivados das séries temporais após a abstração temporal. Cada entrada na matriz representa um evento em uma série temporal específica.\nAgrupamento Local: O agrupamento local refere-se à identificação de subconjuntos de séries temporais que exibem padrões semelhantes em intervalos específicos de tempo. No contexto do Z-Grouping, os agrupamentos locais são identificados em cada canal de rótulo de evento.\nAssociação de Agrupamentos Locais: Associações são identificadas entre agrupamentos locais consecutivos ou sobrepostos que compartilham instâncias de séries temporais semelhantes. O objetivo é descobrir padrões mais amplos e complexos que não seriam detectados apenas nos agrupamentos locais individuais.\nSemigeometric Tiling: Um algoritmo utilizado para identificar padrões ou agrupamentos em matrizes binárias, considerando combinações de intervalos de tempo e contagens de eventos.\n\n\n\n\nFonte: Z. Lee et al. (2022)\n\n\n\nMatriz de Associação e Validação: Uma representação matricial usada para identificar e validar associações entre agrupamentos locais. Essa matriz registra as relações entre os agrupamentos locais e os agrupamentos globais pré-definidos.\n\n\n\n\nFonte: Z. Lee et al. (2022)"
  },
  {
    "objectID": "seminario1/artigo2.html#apresentação-do-algoritmo",
    "href": "seminario1/artigo2.html#apresentação-do-algoritmo",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "2.2 Apresentação do Algoritmo",
    "text": "2.2 Apresentação do Algoritmo\nO algoritmo Z-Grouping é composto por quatro passos distintos, cada um focado em uma etapa específica do processo de análise de séries temporais. São eles:\n\nGeração da Matriz de Sequência de Eventos\nNeste passo, uma coleção de séries temporais é convertida em uma matriz de eventos, utilizando técnicas de abstração temporal como o método SAX. Isso permite uma representação mais simplificada dos dados, facilitando a análise subsequente.\n\n\n\nCode\nX = utils.znorm(X_raw)\nX_sax = utils.SAXify(X, n_bins = 5)\nX_sax[0]\n\n\narray([2, 2, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\n\nCode\nplt.plot(X_raw[0])\nplt.plot(X_sax[0])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCriação de canais de rótulos\nA matriz de eventos em seguida é subdividida em uma matriz binária de mesmo tamanho para cada rótulo\n\n\n\nCode\nmatrices = utils.createChannels(X_sax)\nmatrices[0]\n\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)\n\n\n\nGeração de Agrupamentos Locais\nO próximo passo envolve a identificação de agrupamentos locais em cada canal de rótulo de evento da matriz de eventos. Esse processo é conduzido pelo algoritmo de semigeometric tiling, que busca candidatos a agrupamentos locais com base na contagem de eventos em intervalos de tempo específicos. Além disso, o algoritmo utiliza um parâmetro 𝝰, variando de 0 a 1, para determinar a pureza de um agrupamento local. Por exemplo, ao definir 𝝰 como 0.75, estamos estabelecendo que pelo menos 75% dos elementos do agrupamento devem conter o evento analisado.\nIdentificação de Associações entre Agrupamentos Locais\nNesta etapa, o algoritmo procura associações entre os agrupamentos locais identificados. Isso é feito através da análise de candidatos a associações consecutivas, verificando a proximidade entre elas e identificando instâncias de séries temporais compartilhadas.\nValidação dos Agrupamentos Locais\nPor fim, os agrupamentos locais são validados em relação aos agrupamentos globais pré-definidos. Isso é feito calculando uma pontuação de validade com base na proporção de instâncias de séries temporais em comum e utilizando um parâmetro de densidade para controlar a validade dos agrupamentos locais.\n\n\n\n\nFonte: Z. Lee et al. (2022)\n\n\n\n\n\n\n\n\nFonte: Z. Lee et al. (2022)\n\n\n\n\nFigure 1: Um exemplo dos quatro passos do Z-grouping"
  },
  {
    "objectID": "seminario1/artigo2.html#metodologia-experimental",
    "href": "seminario1/artigo2.html#metodologia-experimental",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "2.3 Metodologia Experimental",
    "text": "2.3 Metodologia Experimental\nA metodologia experimental da pesquisa visa avaliar o desempenho do método Z-Grouping na identificação de agrupamentos locais em séries temporais. Para isso, foi utilizada uma abordagem abrangente que inclui a análise de conjuntos de dados reais de diferentes setores, bem como um conjunto de dados sintético para investigação detalhada dos parâmetros do método. Além disso, alguns métodos foram adaptados para efeitos de comparação. Abaixo, é fornecida uma descrição mais detalhada da metodologia utilizada.\n\nDatasets: Os conjuntos de dados reais utilizados abrangem três setores diferentes: indústria de varejo, mercado de ações e epidemias de COVID-19. Além disso, um conjunto de dados sintético foi gerado para investigação detalhada dos parâmetros do método. Este conjunto de dados sintético simula a presença de similaridade local em meio a padrões sinusoidais com diferentes frequências e amplitudes, além de incorporar ruído e outliers para refletir cenários do mundo real.\nConcorrentes: Como não existe um concorrente direto para o problema, foram feitas adaptações nos métodos semigeometric tiling, kmeans, kmeans-FLEX e kNN para identificar agrupamentos locais em séries temporais.\nProtocolo do Experimento: Para avaliar o desempenho do método Z-Grouping, foi desenvolvido um protocolo de experimento que envolve a divisão dos dados em conjuntos de treinamento e teste. Durante a fase de treinamento, os agrupamentos locais são identificados nos dados de treinamento. Na fase de teste, o objetivo é determinar se os agrupamentos identificados podem identificar padrões de similaridade local em novas instâncias não vistas. Para cada amostra de teste, o agrupamento global correspondente é usado como referência. Isso simula situações do mundo real, como identificar padrões de vendas de um novo produto com base em produtos existentes.\nMétricas de Avaliação: Os resultados são avaliados em termos de erros de predição, como erro quadrático médio (MSE) e erro absoluto médio (MAE), bem como a cobertura dos agrupamentos, ou seja, a fração de séries temporais cobertas pelos agrupamentos identificados."
  },
  {
    "objectID": "seminario1/artigo2.html#análise-crítica-dos-resultados",
    "href": "seminario1/artigo2.html#análise-crítica-dos-resultados",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "2.4 Análise Crítica dos Resultados",
    "text": "2.4 Análise Crítica dos Resultados\nEm relação aos resultados alcançados, todos foram validados 10 vezes, e os seguintes parâmetros foram utilizados:\nα: Este é um parâmetro que controla o nível de “pureza” dos agrupamentos.\nλ: Este parâmetro controla o número de rótulos de abstração que o algoritmo pode usar.\nη: Este parâmetro define o número mínimo de amostras necessárias para que um agrupamento seja considerado válido.\nw: Este é o intervalo de tempo (em número de amostras) que esses algoritmos usam para identificar os agrupamentos locais.\nk: Este é o número de agrupamentos (clusters) que esses algoritmos tentam formar.\nCorte de silhueta: Este é um parâmetro que define um valor de corte para a métrica de silhueta.\nOs algoritmos testados utilizaram os seguintes parâmetros:\nZ-Grouping: α = {0.8, 0.9, 1}, λ = {3, 5, 10}, e η = {1, 1.5, 2}.\nSemigeometric: α = {0.8, 0.9, 1}, e η = {1, 1.5, 2}.\nkmeans: intervalo de tempo w = {30, 60, 180} e k = {3, 5, 10}.\nkNN: intervalo de tempo w = {30, 60, 180} e k = {3, 5, 10}.\nkmeans-FLEX: corte de silhueta de 0,1 até falha em detectar quaisquer agrupamentos válidos.\nEm relação aos resultados no conjunto de dados sintéticos, pode-se analisar através da tabela 2 os erros de teste médios do Z-Grouping e seus quatro competidores. Através dela podemos chegar a algumas conclusões em relação ao Z-Grouping e seus concorrentes:\n\nO Z-Grouping sempre consegue encontrar agrupamentos locais válidos de baixos erros considerando MSE e MAE;\nSemigeometric sofre com sua falta de poder de representação com uma forte suposição binária superado pelo Z-Grouping em relação aos agrupamentos locais;\nkNN atinge seu melhor escore com {w: 180, k: 3};\nkmeans não mostra diferenças notáveis com várias configurações de parâmetros, e é geralmente pior do que seus concorrentes;\nKmeans-FLEX tem seu menor MSE sendo apenas 3,4% menor que o menor erro do kmeans;\nO Semigeometric, kmeans, kNN e kmeans-FLEX são piores do que o Z-Grouping em todas as situações.\n\nAlém disso, utilizando os dados de UCR o Z-Grouping apresentou dificuldade para encontrar padrões para o agrupamento, performando de forma semelhante aos competidores devido a perda de informação pelo SAX em conjuntos mais uniformes.\nJá em relação aos resultados no conjunto de dados reais, com os dados de GARMENT e STOCK o Z-Grouping apresentou resultados com diferença de 44.3% (MSE) e 25.2% (MAE) com cobertura de 88% dos dados, sendo superior aos competidores. E com os dados da COVID pode-se perceber que o trade-off de minimização do erro por perda de cobertura acabou levando o algoritmo a desempenhar com pouca melhora, sacrificando bastante da cobertura, cobrindo apenas 40% dos dados.\nPor fim, ao analisarmos o efeito dos parâmetros, um λ maior pode levar a uma menor cobertura, um α maior leva a agrupamentos mais puros o que permite um número menor de rótulos de evento diferentes, e um η maior exige mais amostras no agrupamento local para validade resultando em menos agrupamentos. Os parâmetros mais altos fazem com que o algoritmo perca sua capacidade de crescer mostrando aproximadamente só 10% de cobertura, além de gerar erros mais altos devido à ausência de pontos de dados para comparação. Por isso as associações dos agrupamentos são utilizadas para aumentar a cobertura preenchendo as lacunas criadas pelos altos valores dos parâmetros.\n\n\n\nTable 1: Erros médios de teste dos algoritmos no banco de dados Sintético (CV: Covarege(%))\n\n\n\n\n\nFonte: Z. Lee et al. (2022)"
  },
  {
    "objectID": "seminario1/artigo2.html#sumário-dos-resultados",
    "href": "seminario1/artigo2.html#sumário-dos-resultados",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "3.1 Sumário dos Resultados",
    "text": "3.1 Sumário dos Resultados\nO Z-Grouping foi testado contra quatro soluções alternativas para o problema de agrupamentos locais, baseadas em adaptações para o problema específico proposto no artigo de abordagens utilizadas de maneira geral em agrupamentos de séries temporais. Os cinco foram avaliados em três datasets com dados do mundo real, um dataset gerado sinteticamente e os 128 datasets clássicos de séries temporais da UCR (University of California, Riverside). O resultado dos experimentos constatou que o Z-Grouping atingiu taxas de erro menores do que seus competidores, e ao mesmo tempo gerou agrupamentos locais sem limitações no tamanho dos intervalos de tempo, o que não pode ser feito utilizando as demais abordagens."
  },
  {
    "objectID": "seminario1/artigo2.html#considerações-finais-e-sugestões-para-futuras-pesquisas",
    "href": "seminario1/artigo2.html#considerações-finais-e-sugestões-para-futuras-pesquisas",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "3.2 Considerações Finais e Sugestões para Futuras Pesquisas",
    "text": "3.2 Considerações Finais e Sugestões para Futuras Pesquisas\nPossíveis abordagens de pesquisas futuras podem incluir o uso de outras funções de abstração temporal (além da SAX, utilizada no artigo), a aplicação de técnicas e heurísticas de otimização global na criação dos agrupamentos locais e o estudo de adaptações do algoritmo para séries temporais multivariadas, isto é, para a geração de agrupamentos multidimensionais."
  },
  {
    "objectID": "seminario1/artigo2.html#instalação",
    "href": "seminario1/artigo2.html#instalação",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "5.1 Instalação",
    "text": "5.1 Instalação\nPara utilizar o Z-Grouping, é necessário ter instaladas as extensões numba, numpy, pyts e o Python em sua versão 3.7 ou superior. Devido às dependências utilizadas, recomenda-se a utilização do Python 3.8 ou superior para evitar conflitos de versão.\nPara instalar o Python 3.8, siga os passos abaixo:\n\nInstale as dependências necessárias:\n\nsudo apt-get install libsqlite3-dev ## (ou sqlite-devel dependendo do SO). \ncd /opt/ \nsudo wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz \nsudo tar -xzf Python-3.8.3.tgz \ncd Python-3.8.3 \nsudo ./configure --enable-optimizations --enable-loadable-sqlite-extensions \nsudo make altinstall \n\nAtive o ambiente com Python 3.8:\n\npython3.8 -m venv ./.venv \nsource .venv/bin/activate \npip install --upgrade pip \npip install numba numpy==1.19.5 pyts matplotlib==3.3.1 \n\nPor fim, para instalar o Z-Grouping, siga estas instruções:\n\ngit clone https://github.com/zedshape/zgrouping.git \ncd zgrouping \nCertifique-se de seguir esses passos com atenção para garantir uma instalação bem-sucedida do Z-Grouping em seu ambiente de desenvolvimento."
  },
  {
    "objectID": "seminario1/artigo2.html#utilização-do-algoritmo-z-grouping",
    "href": "seminario1/artigo2.html#utilização-do-algoritmo-z-grouping",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "5.2 Utilização do Algoritmo Z-Grouping",
    "text": "5.2 Utilização do Algoritmo Z-Grouping\nO algoritmo pode ser facilmente executado importando o método createGroupings do repositório fornecido:\nfrom zgrouping.syntheticGenerator import createSyntheticData \nEle recebe os seguintes parâmetros: * matrices: matriz de labels de evento. Essa matriz pode ser gerada utilizando o método utils.createChannel sobre as séries temporais de entrada. * alpha: o limiar de pureza. * debug: opções de print e debug. * accept: habilita a função de validação da qualidade dos agrupamentos."
  },
  {
    "objectID": "seminario1/artigo2.html#datasets",
    "href": "seminario1/artigo2.html#datasets",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "5.3 Datasets",
    "text": "5.3 Datasets\nO repositório do Z-Grouping já contém bases de dados para teste e avaliação do algoritmo, localizadas na pasta datasets. Algumas das bases de dados incluem: * Covid-19: Base de dados referente ao continente do país, país e contagem de casos de covid no país de 22/01/2020 até 30/09/2021. * Stocks: Base de dados de ações contendo informações como data do registro, valor de abertura do dia, maior valor no dia, menor valor no dia, volume e TAG (nome) da ação.\nAlém disso, o repositório inclui um gerador de bases de dados sintéticas, que cria padrões entre séries temporais. Este gerador pode ser utilizado importando o método createSyntheticData do repositório:\nfrom zgrouping.syntheticGenerator import createSyntheticData\nO método syntheticGenerator.createSyntheticData requer os seguintes argumentos: * c: Número de agrupamentos globais. * tc: Número de membros da instância por agrupamento. * tl: Tamanho de cada série temporal. * no_outliers: Número de outliers. * outlier_size: Tamanho do outlier. * amp: Amplitude. * lineranges: Comprimento de linhas retas. * lineheights: Altura das linhas retas.\nEsses datasets sintéticos podem ser utilizados como substitutos para os datasets reais mencionados anteriormente. No entanto, ainda é necessário passar essas bases pelo método de criação de canais, que é o dado de entrada para o algoritmo de agrupamento."
  },
  {
    "objectID": "seminario1/artigo2.html#exemplo-de-uso",
    "href": "seminario1/artigo2.html#exemplo-de-uso",
    "title": "Artigo 2: Finding Local Groupings of Time Series",
    "section": "5.4 Exemplo de Uso",
    "text": "5.4 Exemplo de Uso\nPara ilustrar o uso do Z-Grouping, apresentamos a seguir um exemplo prático de execução do algoritmo. Os passos a seguir demonstram como gerar dados sintéticos, aplicar transformações e finalmente executar o Z-Grouping para obter os agrupamentos desejados.\nApós a geração e transformação dos dados, será obtida uma matriz com padrões simbólicos, representando séries temporais. A seguir, é apresentado um exemplo dos dados antes e depois da transformação.\n\n\nCode\nfrom zgrouping.zgrouping import grouping, syntheticGenerator, utils \nimport matplotlib.pyplot as plt \n\n# Synthetic generator \ntc = 50 \ntl = 365 \nc = 20 \nno_outliers = 10 \noutlier_size = 10 \n\n# Grouping generation \nn_bins = 5 \nalpha = 0.9 \neta = 1.5 \n\nX_raw, y = syntheticGenerator.createSyntheticData(tc=tc, tl=tl, c=c, no_outliers=no_outliers, outlier_size=outlier_size) \n\n# Normalização e Transformação \nX = utils.znorm(X_raw)  \nX_sax = utils.SAXify(X, n_bins=5) \n\n# Visualização dos dados \nplt.plot(X_raw[150], label='antes') \nplt.plot(X_sax[150], label='depois')\nplt.title('Dados sintéticos antes e depois da transformação')\nplt.legend()\nplt.show()\nprint(X_sax[150]) \n\n\n\n\n\n\n\n\n\n[2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4\n 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3\n 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 2\n 2 2 2 2 2 2 3 3 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 4 3 3\n 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 4 1 1 1 1 1 1 1 1 1 2 1 2 2 2]\n\n\nApós a preparação dos dados, eles serão utilizados como entrada para a criação dos canais do Z-Grouping. A matriz resultante deve ter dimensões \\(S \\times T\\), onde \\(S\\) é o número de séries temporais e \\(T\\) é o tamanho de cada série temporal. Isso significa que cada linha da matriz representa uma série temporal e cada coluna representa o valor daquela série temporal em um determinado tempo.\n\n\nCode\nmatrices = utils.createChannels(X_sax)\n\n\nCom os canais devidamente criados, será utilizado o Z-Grouping para obter os agrupamentos desejados. O código a seguir demonstra como executar o algoritmo e obter os agrupamentos:\n\n\nCode\ngroupings, associations = grouping.createGroupings(matrices, alpha=alpha, accept=False, debug=True) \n\n\n[DEBUG] BEGIN Local grouping generation\n[DEBUG] Generating local grouping candidates from one event label channel - time taken: 44.951890109000004\n[DEBUG] Generating local grouping candidates from one event label channel - time taken: 36.923591759\n[DEBUG] Generating local grouping candidates from one event label channel - time taken: 26.490854166999995\n[DEBUG] Generating local grouping candidates from one event label channel - time taken: 36.19687330400001\n[DEBUG] Generating local grouping candidates from one event label channel - time taken: 29.767859024000018\n[DEBUG] BEGIN Association generation\n\n\nA variável groupings resultante é uma lista de objetos, onde cada objeto representa um agrupamento detectado. Cada agrupamento contém dois campos importantes: * members: um vetor de booleanos indicando se uma série temporal pertence ou não a esse agrupamento, funcionando como uma máscara. * range: o intervalo de tempo no qual o padrão se repete entre as séries temporais.\nAgora, para melhor visualização dos resultados, foi feita uma função que desenha os gráficos de algumas das séries temporais pertencentes a um determinado agrupamento e destaca o padrão detectado.\n::: {#409c2256 .cell execution_count=9} ``` {.python .cell-code} import random\ndef print_3_examples(data, grouping, number_prints): mask = [(index, value) for (index, value) in enumerate(grouping[‘members’])] members = list(filter(lambda tuple : tuple[1], mask))\nrandIndexList = [random.randint(0, len(members) -1) for i in range(number_prints)] \nchoosedMembers = [members[index][0] for index in randIndexList] \n\ninterval = grouping['range'][0:number_prints] \n\nfig, axs = plt.subplots(nrows=number_prints, ncols=1) \nfig.set_figheight(15) \ni = 0 \nfor ax, i in zip(axs, choosedMembers): \n    thisData = data[i] \n    patternData = list(filter(lambda tuple: interval[0] &lt;= tuple[0] &lt;= interval[1], enumerate(thisData))) \n    patternX = [pattern[0] for pattern in patternData] \n    patternY = [pattern[1] for pattern in patternData] \n    ax.plot(thisData) \n    ax.plot(patternX, patternY, 'r') \n    i += 1 \n\nplt.subplots_adjust(hspace=0.0) \nplt.show() \n\nprint(choosedMembers) \nprint_3_examples(X_raw,groupings[13], 6) ```\n::: {.cell-output .cell-output-display}  :::\n::: {.cell-output .cell-output-stdout} [602, 801, 702, 230, 437, 916] ::: :::"
  },
  {
    "objectID": "seminario2/artigo6.html",
    "href": "seminario2/artigo6.html",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "",
    "text": "Mineração de modelos excepcionais (Exceptional Model Mining - EMM) é um framework que tem como objetivo encontrar padrões locais, assim como a descoberta de subgrupos (Sbugroup Discovery - SD), porém EMM busca encontrar correlações entre variáveis alvo que podem ser consideradas interessantes/exepcionais. Já SD busca encontrar padrões, criando seletores nos atributos disponíveis, para encontrar individuos que atendam um padrão de qualidade em relação a distribuição de uma única variável alvo.\nO artigo aborda uma técnica chamada mineração de preferências exepcionais (Excepional Preference Mining - EPM), onde procura-se achar preferência sobre rótulos, ou seja, encontrar preferências excepcionais de diversas opções possíveis sobre uma variável."
  },
  {
    "objectID": "seminario2/artigo6.html#contextualização-do-problema",
    "href": "seminario2/artigo6.html#contextualização-do-problema",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Contextualização do problema",
    "text": "Contextualização do problema\nDiversas linhas de pesquisa buscam construir modelos preditivos capazes de realizarem um ajuste global a um conjunto de dados, de forma fornecer resultados generalizados para todo o conjunto de dados que foi treinado, além de prever com acurácia novos conjuntos de dados. Esses modelos podem fornecer grande ajuda em tomadas de decisão, pois são capazes de preverem uma variável alvo. Contudo, pode-se extrair informações valiosas procurando padrões locais. Em casos como: eleições, market-places, restaurantes, sistemas de recomendação em redes sociais. O objetivo é tentar encontrar padrões de preferência, onde seria possível encontrar atributos de amostras que caracterizam essas preferências.\nBusca-se portanto encontrar correlações entre os rótulos, objetivando encontrar uma preferência entre eles em grupos locais, considerados excepcionais.\n\n\n\nDescoberta de Modelos Excepcionais\n\n\nEntre os métodos que realizam funções semelhantes a EPM, pode-se destacar o uso de regras de associação, proposto por Henzgen e Hullermeier, que faz uma busca por padrões de subranking."
  },
  {
    "objectID": "seminario2/artigo6.html#conceitos-importantes",
    "href": "seminario2/artigo6.html#conceitos-importantes",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Conceitos importantes",
    "text": "Conceitos importantes\n\nLabel Ranking\nDada uma instância a \\(x = \\{a_1, a_2, \\ldots, a_m, \\pi\\}\\) do espaço de instâncias \\(\\mathbb{X}\\), o objetivo é prever o rank dos rótulos \\(\\mathbb{L} = \\{\\lambda_1, \\ldots, \\lambda_n\\}\\) associados com \\(x\\). O ranking pode ser representado como uma strict total order sob \\(\\mathbb{L}\\), definido no espaço de permutação \\(\\Omega\\).\nO Label Ranking se assemelha às tarefas de classificação, mas ao invés de prever uma classe, deseja-se ranquear os rótulos. Na classificação, cada instância é associada a uma distribuição probabilística sob \\(\\Omega\\). Isso significa que, para cada \\(x \\in \\mathbb{X}\\), existe uma distribuição de probabilidade \\(\\mathbb{P}(.|x)\\), de forma que para cada \\(\\pi \\in \\Omega\\), \\(\\mathbb{P}(\\pi|x)\\) é a probabilidade de que \\(\\pi\\) está associado ao ranking de \\(x\\). O objetivo de Label Ranking é mapear \\(\\mathbb{X} \\rightarrow \\Omega\\).\n\n\nDescoberta de Subgrupos e Exceptional Model Mining\nA linguagem usada para retornar um subgrupo é a seguinte:\n\\(𝐴𝑔𝑒 ≥ 30 ⋀ 𝐿𝑖𝑘𝑒𝑠 = 𝑆𝑎𝑙𝑚𝑜𝑛 𝑅𝑜𝑒 → 𝑈𝑛𝑢𝑠𝑢𝑎𝑙\\)\n\nMétrica de Qualidade\nNa mineração de padrões, o quão interessante um padrão pode ser é medido pela sua frequência; já em Subgroup Discovery (SD) essa métrica é estimada de forma supervisionada. Dada uma variável alvo \\(t_1\\) identificada no dataset, o quão interessante um subgrupo nela for, é medido pelo quão não-usual a distribuição dele neste alvo.\nSe em uma população o comum é as pessoas gostarem de sushi chutoro, um subgrupo interessante seria de pessoas que gostam de Makizushi:\n\\(Age ≥ 30 ⋀ Lives in Region = Hokkaido → Makizushi\\)\nSe ao invés de usar um único atributo alvo, múltiplos alvos \\(t_1, \\ldots, t_l\\) estão disponíveis, e se não estiver interessado em descobrir distribuições não usuais em um alvo, mas na interação entre alvos, pode-se então empregar o Exceptional Model Mining (EMM) no lugar do SD. Essa tarefa consiste em dois fatores: Model Class e Métrica de Qualidade.\nModel Class é definido para representar uma interação não comum entre múltiplos alvos que se esteja interessado. Já a métrica de qualidade é usada para definir o que não é usual e, portanto, interessante.\nUm exemplo seria tentar encontrar uma correlação entre a altura de uma pessoa \\(( t_1 )\\) e a altura média dos avós \\(( t_2 )\\). Para isso, é necessário achar um coeficiente de correlação entre $t_1 $ e \\(t_2\\). Nesse caso aplica-se EMM com um correlation model class. No caso de subgrupos muito pequenos, o modelo pode acabar favorecendo-os por serem pouco usuais. Para favorecer subgrupos maiores, deve-se definir uma métrica de qualidade que balanceie o quão excepcional um subgrupo é, e o tamanho dele.\n\n\n\nEstratégia de Busca\nEm EMM é explorado um amplo espaço de busca, guiado por uma métrica de qualidade para expressar a excepcionalidade buscada. Tipicamente, os subgrupos são buscados em uma busca por nível, combinando atributos da mesma forma que é feita uma combinação de itemsets para mineração de padrões frequentes.\nA maioria dos algoritmos de busca, fazem de forma generalista-para-específico, tratando o espaço de busca como um lattice cuja estrutura é definida por um refinement operator \\(\\eta: \\mathbb{D} \\rightarrow 2^\\mathbb{D}\\). Esse operador consegue determinar como descrições podem ser estendidas para descrições mais complexas por adições atômicas. A aplicação proposta no artigo assume que \\(\\eta\\) é um specialization operator: toda descrição $q $ é um elemento do conjunto \\(\\eta(p)\\), o qual é mais especializada que a descrição \\(p\\) em si. O algoritmo, portanto, retorna uma lista ranqueada de descrições que satisfazem as especificações do usuário.\nA estratégia de busca empregada foi um best first search, em cada nível as descrições são ordenadas pela métrica de qualidade \\(\\varphi\\). O limite superior é o grau de complexidade, geralmente limitado por especialistas, para obter descrições com quantidades ideais de atributos para interpretação (profundidade da busca); e o limite inferior é o suporte dos subgrupos.\n\n\nRegras de Distribuições\nRegras de Distribuições (Distribution Rules, DR) é um método de SD para analisar uma única variável alvo. Ao invés de valores representativos (média, desvio-padrão etc.), DR identifica distribuições usuais do alvo, encontrando subgrupos expressados pelas regras de associação com a distribuição consequente:\n\\(S → t = Dist_t|S\\)\n\n\\(S\\): é um conjunto de condições correspondentes à parte antecedente do DR (um subgrupo).\n\\(t\\): é a propriedade de interesse (ou o alvo).\n\\(Dist_t|S\\): é uma distribuição empírica de \\(t\\) quando \\(S\\) é observado. Ela é representada por um conjunto de pares $t_i, freq(t_i) $, onde \\(t_i\\) é um valor particular de \\(t\\) encontrado quando \\(S\\) é observado; \\(freq(t_i)\\) é a frequência de \\(t_i\\) quando os itens de \\(S\\) são observados."
  },
  {
    "objectID": "seminario2/artigo6.html#matriz-de-preferência",
    "href": "seminario2/artigo6.html#matriz-de-preferência",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Matriz de Preferência",
    "text": "Matriz de Preferência\nEm EMP o conceito alvo consiste em um único alvo, o que faz sentido em SD. Contudo, o objeto alvo é um ranqueamento de rótulos, que pode ser representando como comparações em pares. Portanto, representa interações entre múltiplos rótulos individuais, o que é mais consistente no cenário do EMM.\nRanqueamento de rótulos pode ser difícil de analisar e visualizar, quando há uma quantidade grande de rótulos. O apresentado Sushi dataset, que contém 5000 amostras de opiniões de pessoas e 10 tipos de sushi mostra um exemplo real. Até essa quantidade modesta de tipos de sushi diferentes pode ser ranqueado em diversas combinações. Isso pode ter um impacto significativo, onde mais de 98% dos 5000 rankings presentes no dataset são únicos.\nPor conta disso, os autores apresentaram uma alternativa para ranquear os rótulos, introduzindo as Matrizes de Preferência. A matriz de preferência (PM, preference matrix) é uma representação alternativa dos rankings em um conjunto de dados que facilita a análise de comportamentos de preferência excepcionais. Em vez de representar diretamente a ordenação dos rótulos, a matriz de preferência captura as comparações par a par entre rótulos, permitindo uma visão mais detalhada das relações de preferência.\nRepresentar um conjunto de rankings por PM tem certas vantagens sob as tradicionais representações por permutações. PM podem, naturalmente, derivar uma variedade de conjunto métricas para busca de padrões de preferência. Contudo, há certas limitações também, a escolha de agregação de métricas pode esconder informações relevantes nas PMs, por exemplo, escolhendo a média se metade dos rankings minerados forem opostos, o resultando em entradas na PM são iguais a zero. Por isso subgrupos com PM contendo apenas zeros nas entradas são ignorados."
  },
  {
    "objectID": "seminario2/artigo6.html#entendendo-os-algoritmos-usados",
    "href": "seminario2/artigo6.html#entendendo-os-algoritmos-usados",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Entendendo os algoritmos usados",
    "text": "Entendendo os algoritmos usados\nO algoritmo EPM (Exceptional Preference Mining) tem como objetivo identificar subgrupos significativos em um dataset, utilizando uma medida de qualidade para avaliar a diferença entre as preferências do subgrupo e do dataset original.\nfunction EPM(dataset, quality_measure):\n    dataset_pm = compute_preference_matrix(dataset)\n    candidate_subgroups = generate_candidate_subgroups(dataset)\n\n    for subgroup in candidate_subgroups:\n        subgroup_pm = compute_preference_matrix(subgroup)\n        distance = calculate_distance(dataset_pm, subgroup_pm, quality_measure)\n        subgroup.score = calculate_score(distance, subgroup.size)\n\n    ranked_subgroups = rank_subgroups(dataset_pm, candidate_subgroups)\n    significant_subgroups = validate_subgroups(ranked_subgroups)\n\n    return significant_subgroups\n\nPasso a Passo do Algoritmo\nVamos apresentar o algoritmo EPM (Exceptional Preference Mining): 1. Função Principal\n   function EPM(dataset, quality_measure):\nA função principal EPM recebe dois parâmetros:\ndataset: o conjunto de dados a ser analisado. quality_measure: a medida de qualidade utilizada para avaliar as preferências.\n\nComputação da Matriz de Preferência do Dataset\n\ndataset_pm = compute_preference_matrix(dataset)\ncompute_preference_matrix(dataset): Calcula a matriz de preferência para o dataset completo, que será utilizada como referência para comparar os subgrupos.\n\nGeração de Subgrupos Candidatos\n\ncandidate_subgroups = generate_candidate_subgroups(dataset)\ngenerate_candidate_subgroups(dataset): Gera uma lista de subgrupos candidatos a partir do dataset original. Esses subgrupos serão avaliados posteriormente.\n\nAvaliação dos Subgrupos Candidatos Para cada subgrupo na lista de candidatos, realiza-se os seguintes passos:\n\nfor subgroup in candidate_subgroups:\n    subgroup_pm = compute_preference_matrix(subgroup)\n    distance = calculate_distance(dataset_pm, subgroup_pm, quality_measure)\n    subgroup.score = calculate_score(distance, subgroup.size)\ncompute_preference_matrix(subgroup): Calcula a matriz de preferência para o subgrupo específico. calculate_distance(dataset_pm, subgroup_pm, quality_measure): Calcula a distância entre a matriz de preferência do dataset completo e a do subgrupo, utilizando a medida de qualidade fornecida.\n\nFórmula Geral\nA distância geral \\(L_S\\) entre a matriz de preferência do dataset completo \\(M_D\\) e a matriz de preferência do subgrupo \\(M_S\\) é calculada como:\n\\(L_S = \\frac{1}{2} (M_D - M_S)\\)\n\n\nMedidas de Qualidade\n\nNorm\nA medida de qualidade Norm é definida como a norma de Frobenius da matriz \\(L_S\\). A fórmula é:\n\\(\\text{Norm}(S) = \\| L_S \\|_F = \\sqrt{s/n} \\cdot \\sqrt{ \\sum_{i=1}^{k} \\sum_{j=1}^{k} L(i,j)^2 }\\)\n\n\nLabelwise\nA medida de qualidade Labelwise é calculada como o valor máximo entre todas as somas das linhas da matriz \\(L_S\\):\n\\(\\text{Labelwise}(S) = \\max_{i=1,\\ldots,k} \\frac{1}{k(k-1)} \\sum_{j=1}^{k} L(i,j)\\)\n\n\nPairwise\nA medida de qualidade Pairwise é calculada como o valor máximo entre todos os elementos da matriz \\(L_S\\):\n\\(\\text{Pairwise}(S) = \\max_{i,j=1,\\ldots,k} L(i,j)\\)\nEssas medidas são utilizadas para avaliar a diferença entre as preferências do dataset original e dos subgrupos, ajudando a identificar subgrupos excepcionais.\ncalculate_score(distance, subgroup.size): Calcula a pontuação do subgrupo com base na distância calculada e no tamanho do subgrupo.\n\n\n\nPassos para Calcular a Pontuação\nCalcular a Cobertura Normalizada do Grupo\nA Cobertura Normalizada do Grupo é dada por:\n\\(\\sqrt{\\frac{s}{n}}\\)\nonde: - \\(s\\) é o tamanho do subgrupo. - \\(n\\) é o tamanho do dataset completo.\nMultiplicar pela Distância\nA pontuação final do subgrupo é obtida multiplicando a Cobertura Normalizada do Grupo pela Distância calculada na função calculate_distance.\n\nRanqueamento dos Subgrupos\n\nranked_subgroups = rank_subgroups(dataset_pm, candidate_subgroups)\nrank_subgroups(dataset_pm, candidate_subgroups): Ordena os subgrupos candidatos com base na pontuação calculada, gerando uma lista de subgrupos ranqueados.\n\n\nFórmula da Covariância Ponderada\nA fórmula utilizada para calcular a covariância ponderada entre o vetor da matriz de preferência do dataset completo \\(\\text{vec}(M_D)\\) e o vetor da matriz de preferência do subgrupo \\(\\text{vec}(M_S)\\) é:\n\\(\\text{RWCov}(S) = -\\sqrt{\\frac{s}{n}} \\cdot \\text{cov}(\\text{vec}(M_D), \\text{vec}(M_S))\\)\nonde: - \\(\\text{vec}(M_D)\\) é o vetor da matriz de preferência do dataset completo. - \\(\\text{vec}(M_S)\\) é o vetor da matriz de preferência do subgrupo. - \\(\\text{cov}\\) representa a covariância entre os dois vetores. - \\(s\\) é o tamanho do subgrupo. - \\(n\\) é o tamanho do dataset completo.\nEsta medida ajuda a identificar subgrupos que possuem preferências excepcionalmente diferentes em relação ao dataset original.\n\nValidação dos Subgrupos Significativos\n\nsignificant_subgroups = validate_subgroups(ranked_subgroups)\nvalidate_subgroups(ranked_subgroups): Valida os subgrupos ranqueados para identificar aqueles que são estatisticamente significativos.\n\nRetorno dos Subgrupos Significativos Retorno dos Subgrupos Significativos\n\nreturn significant_subgroups"
  },
  {
    "objectID": "seminario2/artigo6.html#metodologia",
    "href": "seminario2/artigo6.html#metodologia",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Metodologia",
    "text": "Metodologia\nPara fazer os experimentos, os autores incorporaram EPM no Cortana, que oferece uma estrutura genérica para descoberta de subgrupos e possui ferramentas para que diferentes abordagens de SD sejam aplicadas. Além disso, eles definiram uma linguagem de descrição para os subgrupos, que é composta por conjunções lógicas de restrições para atributos individuais.\nA estratégia adotada para percorrer o espaço de busca foi uma best-first search gulosa. Sendo que os atributos numéricos foram discretizados com on fly greedy best-first search contendo 8 bins de mesma largura. Os resultados encontrados passaram pela validação DFD, que serve para evitar que subgrupos sejam selecionados por fatores aleatórios. A validação DFD consiste na criação de cópias do dataset, com algumas alterações aleatórias dos atributos-alvo. Subgrupos encontrados nessas cópias podem ser considerados aleatórios. Dessa forma, o método EPM foi aplicado sobre alguns datasets reais. O procedimento DFD permite controlar o problema das múltiplas comparações em SD e EMM, fornecendo uma forma robusta de identificar padrões realmente excepcionais enquanto minimiza a probabilidade de falsas descobertas.\nO procedimento DFD tem apenas um parâmetro: o número de cópias do conjunto de dados. Este número deve ser suficientemente grande para satisfazer certas condições decorrentes da modelagem global envolvida na criação da DFD. Tipicamente, 100 cópias são suficientes, com um nível de significância estatística de 1%.\nSeis bases de dados reais de diferentes domínios foram utilizadas nos experimentos. O dataset Algae apresenta os níveis de frequência de alguns tipos de alga em diferentes rios europeus. O dataset Sushi contém os dados demográficos de um conjunto de pessoas, bem como suas preferências em relação a diferentes tipos de sushi. O dataset Top7movies é um subconjunto de outra base de dados, que apresenta os rankings dos usuários para os 7 filmes mais avaliados. Os datasets GermanElections2005 e GermanElections2009 contém dados socioeconômicos dos distritos administrativos da Alemanha e o ranking de votação dos partidos mais populares nessas regiões. O dataset Cpu-small apresenta dados extraídos de medições relacionadas a servidores.\n\n\n\n\nFonte: sushi\n\n\n\n\n\nFonte: movies\n\n\n\n\n\n\nBases de Dados\n\n\nÉ usado um método chamado Regra de Distribuição para procurar distribuições alvo, ele foi comparado com o RWNorm aplicado no Cortana\n\n\n\nMetodologia técnica DR\n\n\nPara comparar a distribuição dos subgrupos com a populção (o conjunto de dados inteiro) foi realizado o teste estatístico de Komolgorov-Smirgov"
  },
  {
    "objectID": "seminario2/artigo6.html#resultados-obtidos",
    "href": "seminario2/artigo6.html#resultados-obtidos",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Resultados obtidos",
    "text": "Resultados obtidos\nA excepcionalidade informada pelas métricas de qualidade podem se apresentar de diferentes formas em diferentes métricas de qualidade, ou seja, depende o que a métricas de qualidade está procurando. As métricas podem até estar correlacionadas, mas não perfeitamente, os autores forneceram no estudo uma forma do usuário utilizar essas métricas para tomar escolhas mais bem informadas.\n\n\n\nRelação Entre Métricas de Qualidade\n\n\nA figura acima mostra geração de 10.000 subgrupos aleatórios, os quais a pontuação foi avaliada pelas métricas de qualidade apresentadas. A geração aleatória combina descrições até uma profundidade máxima ser alcançada. A profundidade da busca é fixada em 3, permitindo uma boa diversidade de combinações de atributos. Para cada par de métrica de qualidade na figura, há um scatter plot mostrando a relação das pontuações. A primeira linha mostra os subgrupos avaliados por RWNorm e o eixo vertical representa a pontuação dela; o eixo horizontal representa a pontuação de cada métrica de qualidade na seguinte ordem: RWNorm; RWNorm-mode; RWCov; LWNorm; PWMax\nPode-se notar que RWNorm-mode mostra um comportamento distinto, ela é baseada na matriz de distância diferencial L_S, obtida pela diferença entre as modas da população (M_D) e a moda dos subgrupos (M_S). Para essa métrica, se houver uma inversão de preferência no ranking dos rótulos em algum subgrupo de tamanho significativo, ele é considerado interessante, mesmo que seja pouco, como 2%. Para as métricas RWMNorm, LWNorm e PWMax, subgrupos desse tipo já não irão ser interessantes, a não ser que a diferença seja maior, isso fica evidente observando a segunda linha da Figura.\nObservando o RWConv, parece que tem o maior viés, isso se dá pelo fato dessa métrica não se basear na matriz de distância L_S; ao invés disso ela é baseada na correlação negativa entre as matrizes de população (M_D) e subgrupos (M_S). Portanto, essa métrica de qualidade não necessariamente irá encontrar subgrupos que maximizem a preference distance, mas irá mostrar features não usuais de comportamento de forma abstrata.\n\n1. Eleições Alemãs (2005 e 2009) GermanElections2005:\n\nUtilizando a métrica de qualidade PWMax com profundidade de busca 1, foram encontrados 62 subgrupos significativos.\nO subgrupo mais relevante foi “Região = Leste”, onde o partido Esquerda teve mais votos que o partido FDP em todos os 87 distritos da Alemanha Oriental, contrastando com a maioria dos distritos na Alemanha.\nOutro subgrupo significativo mostrou que em regiões de baixa renda (renda ≤ 16.979), o partido Esquerda recebeu mais votos que o partido Verde. GermanElections2009:\nCom as mesmas configurações, foram identificados 57 subgrupos significativos. Novamente, “Região = Leste” mostrou uma forte preferência pelo partido Esquerda em comparação ao partido Verde.\nHouve um aumento no número de distritos de baixa renda favorecendo o partido Esquerda em relação ao partido Verde comparado a 2005. Análise com LWNorm:\nUsando a métrica LWNorm com profundidade de busca 2, foram encontrados 2965 subgrupos significativos.\nO subgrupo “Região = Leste” continuou mostrando forte preferência pelo partido Esquerda.\nOutros subgrupos com características como menor população infantil e maior desemprego também favoreceram o partido Esquerda, enquanto regiões de maior renda mostraram o partido Esquerda como o menos votado.\n\n\n\n2. Top7Movies\n\nUtilizando a métrica LWNorm, foram encontrados 2 subgrupos significativos com profundidade de busca 2.\nO primeiro subgrupo incluía pessoas com mais de 34 anos vivendo abaixo de uma latitude de 32.9, que não gostaram do filme “Beleza Americana” e preferiram “Star Wars: Episódio IV” e “O Resgate do Soldado Ryan”.\nA média de classificação deste subgrupo foi b (Star Wars: Episódio IV) &gt; f (O Resgate do Soldado Ryan) &gt; c (Star Wars: Episódio V) &gt; d (Star Wars: Episódio VI) &gt; g (O Exterminador do Futuro 2) &gt; a (Beleza Americana) &gt; e (Jurassic Park).\n\n\n\n\nResultados Filmes Matriz de Prefixos\n\n\n\n\n3. Algae\n\nUtilizando a métrica RWNorm, os resultados indicam que durante a primavera, as espécies de algas a, b e c são mais comuns em rios.\nA métrica LWNorm revelou mais de 400 subgrupos com profundidade máxima de 2.\nO melhor subgrupo mostrou que a espécie de alga a é fortemente preferida no subgrupo em comparação com o conjunto de dados geral.\nUtilizando profundidade de 3, foram encontrados cerca de 5400 subgrupos, mostrando um comportamento oposto em relação à espécie de alga a.\n\n\n\n4. Sushi\n\nDevido ao alto percentual de rankings únicos, focou-se em padrões de ranking labelwise.\nA métrica LWNorm identificou 149 subgrupos.\nO melhor subgrupo revelou que homens com mais de 30 anos mostraram uma forte preferência por ouriço-do-mar (rótulo e), contrastando com a população geral.\n\n\n\n\nComparação de Subgrupod de Preferência de Sushi com a População\n\n\n\n\n5. Cpu-small\n\nUtilizando a métrica RWCov, foram encontrados 275 subgrupos significativos com profundidade máxima de 4.\nO subgrupo mais relevante exibiu grandes desvios em todas as entradas da matriz de preferência, indicando comportamento de preferência incomum.\n\n\n\n6. Comparação de Métricas de Qualidade\n\nObservou-se uma variação na quantidade de subgrupos obtidos por diferentes métricas.\nA RWNorm apresentou mais subgrupos em comparação a RWNorm-Mode e RWCov. Cada métrica mostrou diferentes vieses, destacando subgrupos específicos com comportamentos de preferência únicos.\nEsses resultados demonstram a eficácia do EPM na identificação de padrões de preferência excepcionais em diversos contextos, proporcionando insights valiosos sobre comportamentos não usuais em rankings.\n\nTambém foi feita uma comparação entre o CORTANA e o CAREN que usava a técnica DR:\n\n\n\nComparação Resultados CAREN vs CORTANA"
  },
  {
    "objectID": "seminario2/artigo6.html#aplicações-e-desafios",
    "href": "seminario2/artigo6.html#aplicações-e-desafios",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Aplicações e desafios",
    "text": "Aplicações e desafios\nA mineração de preferências excepcionais pode ser aplicada em diversos contextos e trazer benefícios. Vamos apresentar abaixo algumas aplicações onde o uso de EPM teria grande valor.\nNa área de negócios, seria possível encontrar subpopulações que possuem preferências divergentes da maioria, o que permitiria a construção de estratégias de marketing direcionado a esses nichos. Além disso, torna-se mais fácil a identificação de variações em tendências de mercado.\nA medicina pode ser positivamente afetada pelo uso de EPM na segmentação de grupos de pacientes com características semelhantes. Isso faz com que cada grupo possa receber um tratamento mais especializado.\nOutro exemplo de aplicação é o setor público. Nesse caso, o conceito de preferência poderia ser usado de forma análoga para identificar subgrupos que possuem necessidades específicas, possibilitando a criação de políticas públicas direcionadas para atender essas pessoas.\nNa área de gestão de recursos humanos, seria interessante utilizar EPM para detectar nichos de funcionários que possuem alguns fatores de motivação e necessidades mais específicos. Assim, a equipe de RH poderia agir de maneira mais assertiva, tornando o ambiente de trabalho mais agradável e inclusivo.\nApesar das possibilidades benéficas citadas acima, é importante ressaltar os impactos negativos que podem surgir com a aplicação da técnica de EPM nesses cenários. O principal impacto identificado é a manipulação de informações. A busca por padrões de preferências que fogem à regra geral pode ser usada de forma indevida caso esses padrões sejam divulgados como se fossem representativos para toda a população. Como um exemplo, pode-se pensar no caso em que uma empresa usa as informações de grupos específicos para promover seus produtos para o público geral, o que configura em uma propaganda enganosa."
  },
  {
    "objectID": "seminario2/artigo6.html#execução-dos-algoritmos-usados-no-artigo",
    "href": "seminario2/artigo6.html#execução-dos-algoritmos-usados-no-artigo",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Execução dos algoritmos usados no artigo",
    "text": "Execução dos algoritmos usados no artigo\nO código-fonte desenvolvido pelos autores para elaboração do artigo não foi disponibilizado. No entanto, foram encontrados dois repositórios no GitHub que utilizam as mesmas técnicas e citam o artigo estudado.\n\nRepositório MD2S_MusicProject\nEste repositório não foi muito explorado pela equipe responsável, pois a documentação que explica os detalhes de implementação foi escrita em francês, o que dificultou o entendimento.\nLink para o repositório\n\n\nRepositório My-Sushi-Addiction\nA implementação contida nesse repositório aplica a EPM em um dataset de preferências de tipos de sushi, mas permite também que outros datasets possam ser utilizados. Diferentemente do artigo, o algoritmo de busca usado nessa versão de EPM foi o beam search. No repositório, a métrica de qualidade é parametrizável, mas apenas a RWNorm está disponível para experimentação. Para utilizar outras métricas, é necessário implementá-las antes de passar como parâmetro.\n\n\n\nBase de dados Sushi 2016\n\n\nLink para o repositório\n\n0 Quick_Start.ipynb: Realiza a execução do projeto em passos simples, bastando apenas seguir o roteiro do próprio notebook. É possível até aplicar a EPM aos próprios dados do usuário, não apenas ao dataset do sushi; ​\n1 Exceptional_Preference_Mining.ipynb: Estruturado como um rascunho, este notebook mostra, passo a passo, como foi implementado o algoritmo de Beam Search e sua aplicação ao conjunto de dados de sushi;​\nbeam_search.py: Funções para o algoritmo de Beam Search;​\npreference_matrix.py: Funções para calcular e visualizar a Matriz de Preferência, e calcular uma pontuação de excepcionalidade derivada delas.\n\nO grupo responsável realizou testes, aplicando o EPM sobre o dataset disponibilizado no repositório. Os principais subgrupos excepcionais descobertos foram os subgrupos de mulheres com menos de 19 anos e homens que responderam a pesquisa muito rapidamente. Também foram feitos experimentos com as outras métricas que não haviam sido previamente disponibilizadas. No geral, os resultados foram semelhantes àqueles vistos com a RWNorm. Uma descoberta notável foi o subgrupo de mulheres com menos de 39 anos de idade com a métrica LWNorm.\n\n\n\nResultados Hacker"
  },
  {
    "objectID": "seminario2/artigo6.html#conclusão",
    "href": "seminario2/artigo6.html#conclusão",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Conclusão",
    "text": "Conclusão\nO artigo analisado apresenta a técnica de Exceptional Preferences Mining (EPM) como uma abordagem inovadora para a descoberta de subgrupos com padrões de preferência excepcionais. Através de uma comparação detalhada com outras técnicas, como SD e EMM, o artigo evidencia a especificidade e a utilidade da EPM. Os experimentos realizados com diversos conjuntos de dados demonstram a eficácia da técnica e suas possíveis aplicações em áreas como negócios, medicina, setor público e gestão de recursos humanos. Apesar dos benefícios apresentados, é importante considerar os impactos negativos potenciais, como a manipulação de informações. A análise dos repositórios relacionados também fornece informações importantes sobre a implementação prática da EPM, evidenciando a adaptabilidade e flexibilidade da técnica."
  },
  {
    "objectID": "seminario2/artigo6.html#referências",
    "href": "seminario2/artigo6.html#referências",
    "title": "Discovering a Taste for the Unusual: Exceptionla Models for Preference Mining",
    "section": "Referências",
    "text": "Referências\nde Sá, C. R., Duivesteijn, W., Azevedo, P., Jorge, A. M., Soares, C., & Knobbe, A. (2018). Discovering a taste for the unusual: exceptional models for preference mining. Machine Learning, 107, 1775-1807."
  },
  {
    "objectID": "seminario2/artigo4.html",
    "href": "seminario2/artigo4.html",
    "title": "Artigo 4: Anytime discovery of a diverse set of patterns with Monte Carlo tree search",
    "section": "",
    "text": "A Descoberta de Subgrupos (SD) é uma área dentro da Mineração de Dados e do Aprendizado de Máquina cujo objetivo é encontrar subgrupos relevantes em conjuntos de dados. Desde o final dos anos 90, muitas estratégias têm sido desenvolvidas para melhorar a eficiência e relevância dos resultados na SD. No entanto, ainda existem desafios significativos, como a eficiência na exploração do espaço de busca.\nO artigo “Anytime discovery of a diverse set of patterns with Monte Carlo tree search” propõe uma solução inovadora: o método MTCS4SD. Esse método utiliza a Árvore de Busca Monte Carlo (Monte Carlo Tree Search - MCTS), uma heurística popular em jogos de tomada de decisão, para descobrir subgrupos de maneira eficiente e diversificada."
  },
  {
    "objectID": "seminario2/artigo4.html#introdução",
    "href": "seminario2/artigo4.html#introdução",
    "title": "Artigo 4: Anytime discovery of a diverse set of patterns with Monte Carlo tree search",
    "section": "",
    "text": "A Descoberta de Subgrupos (SD) é uma área dentro da Mineração de Dados e do Aprendizado de Máquina cujo objetivo é encontrar subgrupos relevantes em conjuntos de dados. Desde o final dos anos 90, muitas estratégias têm sido desenvolvidas para melhorar a eficiência e relevância dos resultados na SD. No entanto, ainda existem desafios significativos, como a eficiência na exploração do espaço de busca.\nO artigo “Anytime discovery of a diverse set of patterns with Monte Carlo tree search” propõe uma solução inovadora: o método MTCS4SD. Esse método utiliza a Árvore de Busca Monte Carlo (Monte Carlo Tree Search - MCTS), uma heurística popular em jogos de tomada de decisão, para descobrir subgrupos de maneira eficiente e diversificada."
  },
  {
    "objectID": "seminario2/artigo4.html#conceitos-e-adaptações",
    "href": "seminario2/artigo4.html#conceitos-e-adaptações",
    "title": "Artigo 4: Anytime discovery of a diverse set of patterns with Monte Carlo tree search",
    "section": "Conceitos e Adaptações",
    "text": "Conceitos e Adaptações\n\nFundamentos do Método\nO MTCS4SD adapta a heurística da Árvore de Busca de Monte Carlo, que se baseia em quatro etapas principais:\n\nSelect: Seleciona recursivamente um nó filho a partir da raiz até encontrar um nó não totalmente expandido, utilizando a métrica UCT.\nExpand: Adiciona aleatoriamente uma nova direção a partir do nó selecionado.\nRollout: Simula aleatoriamente as próximas ações até chegar a um terminal, avaliando a recompensa.\nUpdate: Propaga o resultado obtido para os nós superiores, atualizando os valores de recompensa acumulada e o número de visitas.\n\n\n\n\nEtapas da Árvore de Busca de Monte Carlo.\n\n\n\n\nAdaptação para Descoberta de Subgrupos\nPara o contexto de SD, o método sofre algumas adaptações:\n\nSelect: Utiliza-se a SP-MCTS para selecionar subgrupos promissores, considerando a variância da recompensa.\nExpand: Refinamento para obter descrições mais específicas, evitando descritores fechados e duplicações.\nRollout: Pode executar até que o padrão seja infrequente, agregando os resultados da simulação.\nUpdate: Utiliza a maior propaganda para priorizar as melhores recompensas.\n\nA execução prolongada do algoritmo permite uma busca exaustiva, embora existam hiperparâmetros que limitam o número de execuções."
  },
  {
    "objectID": "seminario2/artigo4.html#metodologia-e-resultados",
    "href": "seminario2/artigo4.html#metodologia-e-resultados",
    "title": "Artigo 4: Anytime discovery of a diverse set of patterns with Monte Carlo tree search",
    "section": "Metodologia e Resultados",
    "text": "Metodologia e Resultados\nO método foi comparado com outros algoritmos da área, como SD-Map*, Beam Search, SSDP e Misere, usando métricas como qualidade média, tempo de execução, redundância, diversidade e uso de memória. Os resultados mostraram que o MCTS4SD encontra conjuntos de padrões mais diversos, embora com um custo um pouco maior de redundância em alguns casos.\n\n\n\nIlustração de diferentes algoritmos de busca. a) Redundancy problem. b) Beam search. c) Sampling exploration. d) MCTS-based exploration.\n\n\n\nResultados das Comparações\n\nBeam Search: MCTS4SD produziu resultados mais diversos.\nSSDP e métodos de amostragem: MCTS4SD garantiu a busca por ótimos locais, algo que esses métodos não conseguem.\nMétodos exaustivos: MCTS4SD obteve resultados onde métodos exaustivos falharam devido a limitações de recursos.\n\nAs vantagens incluem flexibilidade, alta diversidade e convergência para uma busca exaustiva, embora consuma muita memória e explore intensamente o espaço de busca."
  },
  {
    "objectID": "seminario2/artigo4.html#impacto-social-e-considerações-éticas",
    "href": "seminario2/artigo4.html#impacto-social-e-considerações-éticas",
    "title": "Artigo 4: Anytime discovery of a diverse set of patterns with Monte Carlo tree search",
    "section": "Impacto Social e Considerações Éticas",
    "text": "Impacto Social e Considerações Éticas\nA aplicação do MTCS4SD em áreas como saúde, segurança, agricultura e ecologia traz consigo um potencial incrível, mas também levanta questões éticas e sociais importantes. Vamos analisar os benefícios, riscos e responsabilidades de cada área:\n\nSaúde\n\nBenefício: Detectar doenças precocemente, possibilitando tratamentos mais eficazes e personalizados.\nRiscos: Quebra de privacidade dos pacientes, acesso desigual à tecnologia (aumentando a desigualdade social) e decisões automatizadas que desconsideram o lado humano da medicina.\nResponsabilidades: Garantir a segurança dos dados dos pacientes, promover o acesso igualitário à tecnologia e garantir que a tomada de decisão médica continue considerando o paciente como um todo.\n\nSegurança\n\nBenefício: Prevenir crimes de forma mais eficiente, utilizando recursos de forma otimizada.\nRiscos: Reforçar discriminações e vieses existentes nos dados, gerar desconfiança na justiça e ameaçar liberdades individuais.\nResponsabilidades: Assegurar que os dados utilizados sejam neutros e imparciais, operar com transparência e garantir que os direitos individuais sejam preservados.\n\nAgricultura\n\nBenefício: Aumentar a produção de alimentos e otimizar o uso de recursos naturais.\nRiscos: Excluir pequenos agricultores do acesso à tecnologia, aumentar a dependência tecnológica e gerar impactos sociais negativos, como o desemprego.\nResponsabilidades: Promover o acesso à tecnologia de forma democrática, considerar os impactos sociais de sua implementação e garantir uma produção agrícola mais justa e sustentável.\n\nEcologia\n\nBenefício: Gerenciar recursos naturais de forma mais eficiente e proteger o meio ambiente.\nRiscos: Dificultar o acesso à tecnologia devido à complexidade e custos, centralizar o conhecimento e desconsiderar as comunidades locais na tomada de decisão.\nResponsabilidades: Buscar soluções acessíveis e fáceis de usar, democratizar o conhecimento e garantir que as ações beneficiem a todos, principalmente as comunidades mais afetadas pelos problemas ambientais."
  },
  {
    "objectID": "seminario2/artigo4.html#código-aberto-e-documentação",
    "href": "seminario2/artigo4.html#código-aberto-e-documentação",
    "title": "Artigo 4: Anytime discovery of a diverse set of patterns with Monte Carlo tree search",
    "section": "Código Aberto e Documentação",
    "text": "Código Aberto e Documentação\nOs autores disponibilizaram o código do MTCS4DM como software de código aberto no GitHub. O repositório inclui datasets, arquivos executáveis, scripts para experimentos e visualização de resultados.\n\n1. Instalação do Java\nPara executar o programa MCTS4DM, é necessário instalar o Java Development Kit (JDK) e o Java Runtime Environment (JRE) na versão 22. Siga os passos abaixo para realizar a instalação:\n\n1.1. Baixar e instalar o JDK\n\nAcesse o site oficial da Oracle: Oracle JDK 22.\nEscolha a versão adequada, indicamos o JDK 22.0.1, para o seu sistema operacional (Windows, macOS ou Linux) e faça o download do instalador.\nExecute o instalador baixado e siga as instruções na tela para concluir a instalação.\n\n\n\n1.2. Configurar a variável de ambiente JAVA_HOME\n\nNo Windows, abra o Painel de Controle e vá em “Sistema e Segurança” &gt; “Sistema” &gt; “Configurações avançadas do sistema” &gt; “Variáveis de ambiente”.\nCrie uma nova variável de sistema chamada JAVA_HOME e defina seu valor para o caminho onde o JDK foi instalado, por exemplo, C:\\Program Files\\Java\\jdk-20.\nAdicione %JAVA_HOME%\\bin ao PATH nas variáveis de sistema.\n\n\n\n1.3. Verificar a instalação\n\nAbra o terminal (Prompt de Comando no Windows, Terminal no macOS ou Linux) e digite java -version e javac -version para verificar se o Java foi instalado corretamente.\n\n\n\n\n2. Clonar o Repositório\nPara obter o código fonte do MCTS4DM, você precisa clonar o repositório do GitHub. Siga os passos abaixo para clonar o repositório em diferentes sistemas operacionais:\n\n2.1. Windows\n\nBaixe e instale o Git a partir do site oficial: Git para Windows.\nApós a instalação, abra o Git Bash.\nNo Git Bash, execute o seguinte comando para clonar o repositório:\ngit clone https://github.com/guillaume-bosc/MCTS4DM.git\n\n\n\n2.2. macOS\n\nAbra o Terminal.\nInstale o Git usando o Homebrew com o comando:\nbrew install git\nNo Terminal, execute o comando para clonar o repositório:\ngit clone https://github.com/guillaume-bosc/MCTS4DM.git\n\n\n\n2.3. Linux\n\nAbra o Terminal.\nInstale o Git usando o gerenciador de pacotes da sua distribuição Linux, por exemplo, no Ubuntu:\n\nsudo apt - get install git\n\nNo Terminal, execute o comando para clonar o repositório:\ngit clone https://github.com/guillaume-bosc/MCTS4DM.git\n\n\n\n\n3. Executar o MCTS4DM\nDepois de configurar o ambiente Java, é hora de executar o MCTS4DM. Primeiro, certifique-se de que os arquivos de dataset e o arquivo de configuração de parâmetros estejam preparados. Em seguida, siga os passos abaixo:\n\n3.1. Preparar os arquivos de dataset e configuração\n\nNavegue até a pasta datasets no diretório onde o programa MCTS4DM está localizado.\nDentro da pasta datasets, crie uma pasta exclusiva para o dataset, por exemplo, BreastCancer.\nColoque os arquivos properties.csv e qualities.csv dentro da pasta BreastCancer, por exemplo.\nCertifique-se de que o arquivo parameters.conf esteja configurado corretamente, apontando para os caminhos relativos dos arquivos de dataset. O arquivo padrão do repositório parameters.conf explica cada parâmetro e como configurar.\n\n\n\n3.2. Formato dos arquivos de dataset\n\nproperties.csv: Contém a tabela de atributos, onde cada linha é um objeto e cada coluna é um atributo. O separador entre colunas deve ser \\t.\nqualities.csv: Contém a tabela de rótulos, onde cada linha é um objeto (na mesma ordem do properties.csv) e cada coluna é um rótulo de alvo.\n\n\n\n3.3. Executar o MCTS4DM\n\nNo terminal, navegue até a pasta onde está o arquivo MCTS4DM.jar.\nExecute o comando:\njava -jar MCTS4DM.jar &lt;caminho para o arquivo de configuração&gt;\nPor exemplo:\njava -jar MCTS4DM.jar parameters.conf\n\n\n\n\n4. Resultados da Execução\nApós a execução do programa, os resultados estarão disponíveis na pasta especificada no parâmetro resultFolderName dentro da pasta results. Quatro arquivos serão gerados:\n\ninfo.log: Contém os valores dos parâmetros utilizados, o tempo de execução e a data da execução.\nresults.log: Cada linha contém um subgrupo no formato &lt;Descrição&gt;\\t&lt;Alvos&gt;\\t&lt;Medida&gt;\\t&lt;E11&gt;\\t&lt;E10&gt;\\t&lt;E01&gt;. Onde E11 é o número de objetos que respeitam a descrição e estão associados aos alvos, E10 é o número de objetos que respeitam a descrição mas não estão associados aos alvos, e E01 é o número de objetos que não respeitam a descrição mas estão associados aos alvos.\nsupport.log: Contém o suporte de cada subgrupo, onde cada linha corresponde ao suporte do subgrupo correspondente no arquivo results.log.\nsupportE11.log: Contém apenas os IDs dos objetos em E11.\n\n\n\n5. Geração de Plots com Base nos Resultados\nPara gerar plots dos resultados obtidos, siga os passos abaixo:\n\n5.1. Acessar a pasta GenerateDataPlot\n\nNavegue até a pasta GenerateDataPlot no terminal.\n\n\n\n5.2. Executar o comando para gerar plots\n\nExecute o comando:\njava -jar GenerateDataPlot.jar\n\n\n\n5.3. Executar os scripts de bash\n\nNo terminal, vá até a pasta RunExperiments/results.\nExecute os seguintes scripts para gerar os gráficos:\n./launchLength.sh\n./launchQuality.sh\n./launchRuntime.sh\n\n\n\n5.4. Verificar os resultados\n\nOs resultados dos gráficos estarão disponíveis na pasta RunExperiments/results/NbInterations/ com o nome do dataset correspondente.\nOs gráficos gerados estarão em formato .pdf e prontos para análise.\n\n\n\n\n6. Executar Experimentos em Lotes\nA pasta RunExperiments contém o programa que pode executar automaticamente o algoritmo para todos os datasets. Nesta pasta, a pasta results contém os resultados dos lotes, a pasta src contém os códigos-fonte deste programa, o arquivo RunExperiments.jar, e o arquivo paramGen.conf para configuração, assim como é o arquivo parameters.conf. Para executar, use o comando:\njava -jar RunExperiments.jar [options]\nAs opções disponíveis são escolhidas entre {ucb, expand, rollout, memory, update, iter}, que permitem executar o experimento de uma estratégia definida de MCTS.\n\nucb: Executa experimentos usando a estratégia Upper Confidence Bound (UCB).\nexpand: Executa experimentos focando na expansão da árvore.\nrollout: Executa experimentos utilizando a estratégia de rollout.\nmemory: Executa experimentos com foco em otimizações de memória.\nupdate: Executa experimentos aplicando estratégias de atualização.\niter: Executa experimentos através de iterações específicas.\n\nPara iniciar o programa com uma opção específica, use o comando no terminal, por exemplo:\njava -jar RunExperiments.jar ucb\nOs resultados dos experimentos serão armazenados na pasta results dentro de uma hierarquia definida, facilitando a análise dos diferentes experimentos realizados."
  },
  {
    "objectID": "seminario2/artigo4.html#discussões-e-pontos-relevantes",
    "href": "seminario2/artigo4.html#discussões-e-pontos-relevantes",
    "title": "Artigo 4: Anytime discovery of a diverse set of patterns with Monte Carlo tree search",
    "section": "Discussões e Pontos Relevantes",
    "text": "Discussões e Pontos Relevantes\nDurante a discussão do seminário, vários pontos foram levantados:\n\nViés: A aplicação em segurança pública gerou preocupações sobre o potencial de reforçar vieses existentes. É crucial garantir a justiça e a equidade na aplicação do método.\nAvaliação da Qualidade dos Grupos: A forma como a qualidade dos grupos é avaliada no artigo gerou debate, levantando a necessidade de métricas mais robustas e transparentes.\nComparação com Abordagens Numéricas: A falta de comparação com métodos numéricos foi apontada como uma limitação do estudo.\nImportância da Diversidade: A capacidade do MCTS4DM em encontrar subgrupos diversos foi destacada como um diferencial importante em relação a outros métodos."
  },
  {
    "objectID": "seminario2/artigo4.html#conclusões",
    "href": "seminario2/artigo4.html#conclusões",
    "title": "Artigo 4: Anytime discovery of a diverse set of patterns with Monte Carlo tree search",
    "section": "Conclusões",
    "text": "Conclusões\nO artigo apresenta o MTCS4SD como uma abordagem promissora para a descoberta de subgrupos, demonstrando vantagens em relação a métodos tradicionais em termos de qualidade, diversidade e flexibilidade.\nNo entanto, é importante destacar que a implementação deste método traz consigo questões éticas e sociais importantes, especialmente quando aplicado em áreas sensíveis como saúde, segurança, agricultura e ecologia. Portanto, ao utilizar o MTCS4DM, é crucial considerar esses aspectos e garantir que o método seja usado de maneira responsável e ética.\nPor fim, a disponibilização do código-fonte e a descrição detalhada do processo de execução fornecem uma base sólida para futuras pesquisas e desenvolvimentos na área de Descoberta de Subgrupos. Isso abre caminho para novas possibilidades de exploração e aprimoramento do MCTS, contribuindo para o avanço contínuo do campo de Aprendizado de Máquina."
  },
  {
    "objectID": "seminario2/artigo4.html#referências",
    "href": "seminario2/artigo4.html#referências",
    "title": "Artigo 4: Anytime discovery of a diverse set of patterns with Monte Carlo tree search",
    "section": "Referências",
    "text": "Referências\n\nBosc, G., Boulicaut, JF., Raïssi, C. et al. Anytime discovery of a diverse set of patterns with Monte Carlo tree search. Data Min Knowl Disc 32, 604–650 (2018). Link para o artigo"
  },
  {
    "objectID": "seminario1/artigo3.html",
    "href": "seminario1/artigo3.html",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "",
    "text": "A mineração de dados é uma área de estudo e pesquisa cada vez mais frequente devido ao seu impacto e capacidade de extração de informações relevantes em grandes conjuntos de dados. Nesse sentido, uma das principais subáreas de mineração de dados é a de mineração de subgrafos frequentes.\nEm termos gerais, dado um grafo, essa área de pesquisa se concentra em descobrir algoritmos e heurísticas que permitem analisar quais são os subgrafos de maior importância (motif) e que ocorrem com maior frequência dentro desse mesmo grafo. Sabe-se que esse é um problema de natureza difícil, devido à alta explosão combinatória do espaço de solução. Por isso, muitos estudos são feitos para desenvolver uma abordagem eficiente de solução do problema."
  },
  {
    "objectID": "seminario1/artigo3.html#contextualização-do-problema",
    "href": "seminario1/artigo3.html#contextualização-do-problema",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "Contextualização do Problema",
    "text": "Contextualização do Problema\nA motivação para desenvolver soluções nesse tipo de análise de dados é diversa. Dentre as áreas em que é possível aplicar essa técnica, podemos destacar: a biologia e a química, no estudo e pesquisa de fármacos e das relações entre átomos e ligações químicas; o setor de transporte, na análise de rotas mais eficientes e baratas para deslocamento; assim como redes de computadores, no estudo das conexões entre diferentes computadores e servidores de modo a encontrar redistribuições mais rápidas e eficientes de pacotes, entre outras."
  },
  {
    "objectID": "seminario1/artigo3.html#algoritmos-clássicos",
    "href": "seminario1/artigo3.html#algoritmos-clássicos",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "Algoritmos Clássicos",
    "text": "Algoritmos Clássicos\nOs algoritmos clássicos de mineração de subgrafos frequentes baseiam-se em abordagens similares àquelas usadas na mineração de conjuntos de itens frequentes. Em resumo, consistem na geração de candidatos e no crescimento de padrões para encontrar os subgrafos recorrentes na base de dados.\nTais abordagens apresentam tanto vantagens como desvantagens, dentre as quais vale destacar:\n\n\n\n\n\n\n\n\n\nGeração de Candidatos\nCrescimento de Padrões\n\n\n\n\nVantagens\n- Simples\n- Redução do espaço de busca por meio de podas\n\n\n\n- Natural\n- Eliminação de redundâncias\n\n\n\n\n\n\n\nDesvantagens\n- Explosão combinatória do número de candidatos\n- Alto custo na verificação dos subgrafos\n\n\n\n- Extremamente ineficiente em grandes grafos\n- Ineficiente em grandes grafos\n\n\n\n- Consome grande quantidade de recursos como memória"
  },
  {
    "objectID": "seminario1/artigo3.html#spminer-uma-abordagem-inovadora",
    "href": "seminario1/artigo3.html#spminer-uma-abordagem-inovadora",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "SPMiner: Uma abordagem inovadora",
    "text": "SPMiner: Uma abordagem inovadora\nCom o problema contextualizado, fica evidente a importância da pesquisa conduzida pelos autores do artigo. A abordagem proposta por eles consiste no uso de aprendizado profundo para tornar a computação dos subgrafos frequentes eficiente. A ideia principal é utilizar uma Rede Neural em Grafos (GNN) para mapear subgrafos para um espaço ordenado de embeddings multidimensional de modo que a busca por subgrafos frequentes nesse espaço seja mais rápida.\nA construção da rede neural foi feita justamente com esse objetivo, de modo que a arquitetura e a função de perda utilizada garantam que a relação de ordem parcial entre subgrafos seja mantida. Em termos gerais, se um subgrafo A é subgrafo de B, então A se encontra abaixo e à esquerda de B no espaço de embeddings citado."
  },
  {
    "objectID": "seminario1/artigo3.html#o-algoritmo-do-spminer",
    "href": "seminario1/artigo3.html#o-algoritmo-do-spminer",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "O Algoritmo do SPMiner",
    "text": "O Algoritmo do SPMiner\nIdentificar subgrafos frequentes de importância, também chamados de Redes Funcionais (Network Motif) é crucial para analisar e prever propriedades de redes do mundo real. Contudo, encontrar grandes redes funcionais comuns é um problema desafiador não apenas devido à sua sub-rotina NP Difícil de contagem de subgrafos, mas também ao crescimento exponencial do número de possíveis padrões de subgrafos.\nO algoritmo SPMiner alia o poder das seguintes áreas: redes neurais de grafos, espaços latentes ordenados (order embedding space) e uma estratégia de busca eficiente no espaço de possibilidades. Isso possibilita a identificação de padrões de subgrafos de rede que aparecem com mais frequência no grafo de destino.\nPara tal, de forma simplificada e ordenada, ele segue os seguintes passos:\n\nDecompõe o grafo de destino em subgrafos sobrepostos ancorados;\nMapeia cada subgrafo do passo anterior em um espaço multidimensional latente ordenado;\nUtiliza um caminhamento monotônico no espaço resultante do passo anterior;\nIdentifica as Redes Funcionais frequentes.\n\nO algoritmo não é exato, porém o tempo de execução é mais de 100 vezes menor do que os algoritmos exatos e é preciso para subgrafos pequenos. Como limitação ele não é capaz de retornar a frequência dos elementos. De uma maneira geral ele representa uma inovação na área e sua estrutura básica pode inspirar na busca de soluções para outros problemas de mesma magnitude, a exemplo dos combinatoriais.\n\n\n\nDiagrama representativo do encoder e decoder do SPMiner\n\n\nAbaixo segue a descrição detalhada de cada passo.\nA decomposição do grafo inicial é baseada na definição de redes funcionais ancoradas. Define-se \\(G_T\\) o grafo inicial, decomposto em seus vértices \\(V_T\\) e arestas \\(E_T\\) da seguinte forma \\(G_T=(V_T,E_T)\\). Analogamente, define-se \\(G_Q=(V_Q,E_Q)\\) como sendo a busca de uma rede funcional. O problema é determinar se uma cópia isomórfica de \\(G_Q\\) aparece em \\(G_T\\), ou seja, se existe uma função injetora entre os vértices e arestas de ambos.\nA definição de um subgrafo ancorado é dada da seguinte forma: Seja \\((G_Q, v)\\) um padrão de subgrafo ancorado no vértice \\(v\\). A frequência do motivo \\(G_Q\\) no conjunto de dados do grafo \\(G_T\\), em relação à âncora \\(v\\), é o número de vértices \\(u\\), em \\(G_T\\) para o qual existe um isomorfismo de subgrafo \\(f: V_Q→ V_T\\) tal que \\(f(v) = u\\).\nDefine-se a frequência como o número de subconjuntos exclusivos de vértices \\(S ⊂ V_T\\) para onde existe um isomorfismo de subgrafo \\(f : V_Q → V_T\\) cuja imagem é \\(S\\). Comparado com estado da arte, esta medida é mais robusta a outliers, provê uma visão holística e satisfaz a propriedade de Downward Closure Property (DCP).\n\n\n\nDiferença entre a frequência de subgrafos ancorados em nós e em nível de grafo.\n\n\nNa imagem acima, encontram-se no grafo à esquerda uma frequência de \\(\\binom{100}{6}\\) subgrafos isomórficos ao grafo da direita e uma medida ancorada de 1.\nPortanto, dado um grafo \\(G_T\\), um parâmetro de tamanho de subgrafo \\(K\\) e o número desejado de resultados \\(R\\), o objetivo do SPMiner é identificar, dentre todos os possíveis grafos de \\(K\\) vértices, os \\(R\\) subgrafos com a maior frequência em \\(G_T\\).\nDadas as definições, a decomposição de \\(G_T\\) é feita extraindo os k-hop vizinhos \\(G_V\\) ancorada em cada vértice \\(v\\), ou seja, os que contém todos os vértices que têm o caminho mais curto de no máximo \\(k\\) para o vértice \\(v\\).\nO processo de mapeamento para o espaço latente ordenado é feito por uma rede neural utilizando o nó âncora como uma feature categórica. O SPMiner usa essa Rede Neural de Grafo (GNN) para aprender uma função de incorporação φ, que mapeia os vizinhos ancorados em nós em pontos no espaço latente tal que a propriedade do subgrafo é preservada. É importante ressaltar que o SPMiner GNN é treinado apenas uma vez e depois pode ser aplicado a qualquer grafo de destino em qualquer domínio.\nPara caminhar no espaço latente, o trabalho sugere três estratégias: a heurística gulosa, a busca em feixes e o algoritmo Monte Carlo Tree Search (MTCS). Para a estratégia gulosa, o trabalho apresenta a função de perda que será avaliada. No algoritmo de busca em feixe, concilia-se a busca em profundidade com a estratégia gulosa. Já para o algoritmo MTCS, o trabalho apresenta uma função objetivo com base no critério superior de confiança para árvores (UCT).\nDessa forma, dado o resultado da exploração do espaço, o algoritmo informa os subgrafos mais frequentes."
  },
  {
    "objectID": "seminario1/artigo3.html#metodologia-experimental",
    "href": "seminario1/artigo3.html#metodologia-experimental",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "Metodologia Experimental",
    "text": "Metodologia Experimental\nO SPMiner foi comparado com outros métodos aproximativos e, quando viável, com métodos de enumeração exata. As comparações podem ser divididas em 3 grupos: subgrafos pequenos, subgrafos grandes plantados e subgrafos grandes reais.\nPara os subgrafos pequenos, a comparação com métodos de enumeração exata é possível, o que torna a comparação de desempenho mais sólida. Os subgrafos grandes plantados são subgrafos gerados artificialmente. A ideia é ter um parâmetro mais realista do desempenho na prática. A abordagem é vantajosa, pois combina grafos maiores, que são o conjunto de interesse para a aplicação, e permite avaliar o desempenho de forma mais objetiva. A última abordagem, a de comparação com grafos grandes reais, contrasta com os métodos aproximativos, mas é a que melhor aproxima o uso prático. Os datasets utilizados nessa fase incluem aplicações em diversos domínios: biologia (ENZYMES), química (COX2) e imagens (MSRC).\nTambém foi feita uma comparação do tempo de execução. Foram usados dois algoritmos como baseline: o MFInder (Kashtan et al., 2004) e o RAND-ESU (Wernicke, 2006). Os parâmetros foram adequados para se obter um número comparável de subgrafos e tempo de execução."
  },
  {
    "objectID": "seminario1/artigo3.html#resultados",
    "href": "seminario1/artigo3.html#resultados",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "Resultados",
    "text": "Resultados\nO SPMiner tem uma acurácia significativa na identificação de motifs nos subgrafos pequenos. O SPMiner encontra motifs de tamanho 5 e 6 em até 90% das vezes. A comparação de tempo de execução é drástica: o método de enumeração ESU (Wernicke, 2006) demora cerca de 10 horas, enquanto o SPMiner executa em apenas 5 minutos. Similarmente, para os subgrafos plantados, o desempenho também foi satisfatório.\n\n\n\nComparação entre SPMiner e principais algoritmos aproximados de mineração de subgrafos. Os 10 principais motifs identificados pelo SPMiner têm frequência mais alta do que aqueles encontrados pelas baselines, para motifs de tamanho 5 e tamanho 6\n\n\n\n\n\nComparação das frequências medianas de motifs identificados por diferentes estratégias de busca e baselines.O SPMiner encontra padrões com uma frequência ancorada em nós mais alta do que as baselines MLP neural ou as baseadas em amostragem Rand-ESU e MFinder, nos conjuntos de dados COX2 (A), ENZYMES (B) e ENZYMES (C)\n\n\n\n\n\nFrequência de motifs identificados por Gaston, gSpan, Motivo e SPMiner. O SPMiner é capaz de identificar motifs de alta frequência de grande tamanho\n\n\n\n\n\nComparação de tempo de execução entre métodos não neurais e o SPMiner\n\n\nPara os subgrafos grandes reais, a identificação dos motifs foi de 10 a 100 vezes mais frequente. Além disso, o SPMiner consegue identificar motifs grandes, com 10 vértices ou mais, enquanto a mediana dos baselines é 3. Uma das vantagens do SPMiner é o pré-treinamento, que é executado apenas uma vez e é generalizável para qualquer grafo. Isso reduz drasticamente o tempo de execução, em particular se comparado aos métodos exatos.\nApesar das vantagens, o algoritmo também possui suas limitações. O treino com grafos aleatórios pode limitar sua aplicabilidade em análise de bancos de dados reais. Além disso, o algoritmo retorna apenas os k motifs mais frequentes, sem considerar a ordenação."
  },
  {
    "objectID": "seminario1/artigo3.html#aplicações-e-desafios",
    "href": "seminario1/artigo3.html#aplicações-e-desafios",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "Aplicações e Desafios",
    "text": "Aplicações e Desafios\nO uso de algoritmos de recuperação de informação relacionados a subgrafos é amplamente difundido no meio industrial, acadêmico e social, tais como: 1. no meio industrial, destacam-se a indústria química e farmacêutica na produção de elementos químicos específicos e fármacos; 2. na engenharia de software, destaca-se a sua utilização no desenvolvimento de fluxos de controle de aplicações; 3. no comércio eletrônico, têm-se os sistemas de recomendação; 4. no setor bancário, atua na detecção de fraudes, 5. na cadeia de suprimentos, detecta rotas e identifica padrões logísticos; 6. no meio acadêmico, a área da bioinformática destaca-se em questões ligadas a redes de proteínas e conjuntos de proteínas; 7. na ciência ambiental, auxilia na identificação de habitats críticos para a conservação; 8. no meio social, destaca-se detecção de rotas frequentes para planejamento urbano, novos negócios e definição de rotas de fuga.\nO algoritmo também possui tais desvantagens: 1. pela análise de redes sociais é possível propagar informações maliciosas, perda de privacidade e perda da autonomia individual; 2. na área comercial é possível induzir o consumo desnecessário.\nDentre os usos específicos do algoritmo, incluem-se artigos de aplicação para a detecção de rumores ou notícias falsas em redes sociais (Detecting rumours with latency guarantees using massive streaming data). E um novo algoritmo chamado Multi-SPMiner já foi proposto (Multi-SPMiner: A Deep Learning Framework for Multi-Graph Frequent Pattern Mining with Application to spatiotemporal Graphs)."
  },
  {
    "objectID": "seminario1/artigo3.html#como-usar-o-spminer",
    "href": "seminario1/artigo3.html#como-usar-o-spminer",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "Como Usar o SPMiner",
    "text": "Como Usar o SPMiner\nJure Leskovec, professor da Universidade Stanford e um dos autores do artigo que apresenta o SPMiner, comanda um projeto denominado Stanford Network Analysis Platform (SNAP). O projeto consiste no desenvolvimento e manutenção de um sistema de código aberto para a análise de redes complexas. Um dos módulos do SNAP, é o Neural Subgraph Learning (NSL), que consiste em uma biblioteca com várias rotinas dedicadas ao aprendizado de relações de subgrafos, e um dos algoritmos implementados é o SPMiner.\nA fim de solucionar problemas de compatibilidade de bibliotecas, conflitos de instalação e facilitar a execução em ambientes diferentes, foram feitas modificações na implementação disponibilizada pelo SNAP e gerado um arquivo para a criação de um ambiente Docker. O repositório completo, que inclui um pequeno tutorial para a execução do SPMiner via Docker, pode ser acessado neste link.\nAlém da implementação do SPMiner, os desenvolvedores do projeto SNAP também disponibilizaram datasets que podem ser utilizados para testar o funcionamento do algoritmo, além de scripts para diferentes análises dos resultados. Experimentos locais utilizando o dataset COX2, executando o SPMiner, sem GPU, obtiveram tempo de execução próximo de 40 minutos.\n\n\n\nSubgrafos frequentes de tamanho 5, 12 e 20, respectivamente, resultantes da mineração de padrões no dataset COX2, composto por 467 grafos. A mineração foi realizada utilizando a estratégia de pesquisa Greedy, com padrões identificados tendo tamanho mínimo de 5 e máximo de 20. O processo de mineração levou 40 minutos e 17 segundos\n\n\n\nDados de entrada\nHá alguns datasets no repositório, mas para uma aplicação real, o usuário pode alterar o arquivo de entrada, bem como outros parâmetros: tamanho mínimo e/ou máximo dos subgrafos frequentes, estratégia de pesquisa, dentre outros.\nOs dados de entrada estão no formato txt:\n\nA representação de cada grafo é inicializada por uma linha do tipo:\nt # {id do grafo}\nEm seguida, n linhas, cada uma representa um vértice:\nv {id do vértice} {rótulo do vértice}\nE por fim, m arestas, novamente, uma por linha:\ne {id do vértice de origem} {id do vértice de destino} {rótulo da aresta}\n\nContudo, o código aceita outras estruturações dos grafos (por exemplo, os dados do TUDataset tem um .txt com uma lista de adjacência, entre outros adicionais).​"
  },
  {
    "objectID": "seminario1/artigo3.html#conclusão",
    "href": "seminario1/artigo3.html#conclusão",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "Conclusão",
    "text": "Conclusão\nO SPMiner representa um avanço significativo para a mineração de subgrafos frequentes, oferecendo uma abordagem inovadora e eficiente para a identificação de padrões em dados complexos. Com seu potencial de aplicação em uma variedade de domínios, o SPMiner promete abrir novas oportunidades de pesquisa e inovação. Ao aproveitar o poder do aprendizado profundo, o SPMiner por auxiliar na revelação de padrões ocultos e oferecer insights valiosos."
  },
  {
    "objectID": "seminario1/artigo3.html#referências",
    "href": "seminario1/artigo3.html#referências",
    "title": "Artigo 3: Representation Learning for Frequent Subgraph Mining",
    "section": "Referências",
    "text": "Referências\n\nYing, R., Fu, T., Wang, A., You, J., Wang, Y., & Leskovec, J. (2024). Representation Learning for Frequent Subgraph Mining. arXiv preprint arXiv:2402.14367. https://doi.org/10.48550/arXiv.2402.14367​\nSIMPLEDATAMINING. Graph Pattern Mining (gSpan) - Introduction. Disponível em: https://simpledatamining.blogspot.com/2015/03/graph-pattern-mining-gspan-introduction.html. Acesso em: 19 abr. 2024.​\nJ, Vamsi. GRAPH MINING. Disponível em: https://www.youtube.com/watch?v=KoG5lEAJmgI&t=1694s. Acesso em: 18 abr. 2024.​\nRex Ying et al. Frequent Subgraph Mining by Walking in Order Embedding Space. In: INTERNATIONAL CONFERENCE ON MACHINE LEARNING, 37., 2020, Virtual. Workshop details. Disponível em: https://icml.cc/virtual/2020/7061. Acesso em: 22 abr. 2024.​\nFrequent Subgraph Mining by Walking in Order Embedding Space. R. Ying, A. Wang, J. You, J. Leskovec, 2020. Disponível em:https://snap.stanford.edu/frequent-subgraph-mining/​\nNguyen, T.T., Huynh, T.T., Yin, H. et al. Detecting rumours with latency guarantees using massive streaming data. The VLDB Journal 32, 369–387 (2023). https://doi.org/10.1007/s00778-022-00750-4​\nZEGHINA, Assaad et al. Multi-SPMiner: A Deep Learning Framework for Multi-Graph Frequent Pattern Mining with Application to spatiotemporal Graphs. Procedia Computer Science, v. 225, p. 1094-1103, 2023. ISSN 1877-0509. Disponível em: https://doi.org/10.1016/j.procs.2023.10.097. Acesso em: 22 abr. 2024."
  },
  {
    "objectID": "seminario1.html",
    "href": "seminario1.html",
    "title": "Introdução",
    "section": "",
    "text": "Os seminários da disciplina consistem em uma apresentação coletiva (da turma) de um artigo mais recente sobre tópicos diretamente relacionados ao conteúdo visto em sala. A ideia é que a turma como um todo estude os artigos mais recentes da área e discuta esses trabalhos em sala. Em cada sessão, discutimos três artigos recentes relacionados aos tópicos abordados nas aulas teóricas.\nPara isso, adotamos um formato adaptado da proposta apresentada pelos Profs. Alec Jacobson e Colin Raffel, ambos da Universidade de Toronto (Canadá) – veja a proposta original em https://colinraffel.com/blog/role-playing-seminar.html. A proposta consiste em fazer uma encenação de papéis (role play) científicos para a apresentação do seminário. Nessa proposta, cada grupo cumprirá um papel na apresentação. Ao final, uma apresentação em formato de slides e um documento textual são produzidos. A apresentação é usada em sala de aula para fomentar as discussões, enquanto o documento fornece uma descrição textual das impressões da turma com a intenção de descrever o tema do artigo para um público amplo interessado em aprendizado de máquina e mineração de dados.\nSão apresentados a seguir os artigos discutidos no semestre 2024/1, com os respectivos links para os slides e documentos textuais apresentando os artigos.\n\n\nA Survey of High Utility Itemset Mining\nby Fournier-Viger, Philippe, Jerry Chun-Wei Lin, Tin Truong-Chi, and Roger Nkambou. 2019\nhttps://doi.org/10.1007/978-3-030-04921-8_1\n\n\n\nFinding Local Groupings of Time Series\nby Lee, Zed, Marco Trincavelli, and Panagiotis Papapetrou. 2023\nhttps://doi.org/10.1007/978-3-031-26422-1_5\n\n\n\nRepresentation Learning for Frequent Subgraph Mining\nby Ying, Rex, Tianyu Fu, Andrew Wang, Jiaxuan You, Yu Wang, and Jure Leskovec. 2024\nhttps://arxiv.org/abs/2402.14367"
  },
  {
    "objectID": "seminario1.html#artigo-1",
    "href": "seminario1.html#artigo-1",
    "title": "Introdução",
    "section": "",
    "text": "A Survey of High Utility Itemset Mining\nby Fournier-Viger, Philippe, Jerry Chun-Wei Lin, Tin Truong-Chi, and Roger Nkambou. 2019\nhttps://doi.org/10.1007/978-3-030-04921-8_1"
  },
  {
    "objectID": "seminario1.html#artigo-2",
    "href": "seminario1.html#artigo-2",
    "title": "Introdução",
    "section": "",
    "text": "Finding Local Groupings of Time Series\nby Lee, Zed, Marco Trincavelli, and Panagiotis Papapetrou. 2023\nhttps://doi.org/10.1007/978-3-031-26422-1_5"
  },
  {
    "objectID": "seminario1.html#artigo-3",
    "href": "seminario1.html#artigo-3",
    "title": "Introdução",
    "section": "",
    "text": "Representation Learning for Frequent Subgraph Mining\nby Ying, Rex, Tianyu Fu, Andrew Wang, Jiaxuan You, Yu Wang, and Jure Leskovec. 2024\nhttps://arxiv.org/abs/2402.14367"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizado Descritivo",
    "section": "",
    "text": "Aprendizado Descritivo — DCC/UFMG   Prof. Renato Vimieiro \n\nEssa disciplina é ofertada no Programa de Pós-Graduação em Ciência da Computação da Universidade Federal de Minas Gerais. Ela tem como objetivo apresentar técnicas avançadas para identificação de padrões descritivos em bases de dados. A(o) aluna(o) terá contato com técnicas para aprendizado de padrões não-supervisionados e supervisionados. Serão discutidas as dificuldades computacionais da busca por tais padrões, bem como sua utilidade para análise exploratória de dados.\nOs tópicos abordados na disciplina são:\n\nDiferenças entre aprendizado descritivo e preditivo.\nAprendizado descritivo não-supervisionado.\nAprendizado descritivo supervisionado.\nRepresentações condensadas.\nMétricas de qualidade de padrões descritivos.\nAlgoritmos de aprendizado de padrões descritivos supervisionados e não-supervisionados.\nEstudos de casos e aplicações em problemas reais.\n\nOs tópicos são apresentados através de aulas expositivas sobre o assunto, leitura e apresentação de seminários sobre artigos recentes na literatura, e projetos de aplicação dos métodos estudados para extração de conhecimento de bases de dados.\nUtilize o menu acima ou o link a seguir para visualizar o conteúdo produzido nos seminários e projetos desenvolvidos pelos alunos.\n\nSeminários: Padrões frequentes; Descoberta de subgrupos; Aplicações"
  },
  {
    "objectID": "seminario1/artigo1.html",
    "href": "seminario1/artigo1.html",
    "title": "Artigo 1: A Survey of High Utility Itemset Mining",
    "section": "",
    "text": "No campo da mineração e análise de dados, a mineração de padrões frequentes possui uma relevância bastante significativa, sendo importante para encontrar associações e correlações entre diferentes variáveis. Essa área de estudo surgiu juntamente com o crescimento exponencial da quantidade de dados disponíveis em diversos setores da vida cotidiana, em especial o comércio, tendo sido batizada com o nome de “Analise da cesta de compras”.\nPara dar seguimento a este artigo, é necessário definir (ou relembrar) alguns conceitos:\nA análise final dos padrões identificados como frequentes através da mineração executada pode muitas vezes ser complicada, por se tratar de uma técnica de aprendizado não supervisionado. Mas além da complexidade inata da análise de resultados, existe também a possibilidade dos padrões obtidos não significarem nada, ou simplesmente serem pouco úteis para os objetivos do negócio.\nPor exemplo, considere que na análise de uma papelaria os itens “lápis” e “borracha” são frequentemente incluídos em uma mesma transação, o preço final pago pelo consumidor por apenas esse conjunto de itens será muito baixo, não trazendo os benefícios esperados da análise. Porém, na mesma papelaria, pode ser que os itens impressora e cartucho de tinta também são frequentemente comprados juntos, o que leva a um preço final maior pago pelo consumidor.\nO exemplo anterior é básico, mas ilustra a ideia de que os itens serem apenas frequentes pode não ser o suficiente, sendo necessário que as combinações analisadas sejam também úteis para o analista. É nesse contexto que surge a mineração de padrões frequentes de alta utilidade, tratada no artigo “A Survey of High Utility Itemset Mining”, que aborda diferentes algoritmos para resolver esse problema.\nPara entender esses algoritmos, é primeiro necessário fazer uma segunda leva de definições sobre o assunto, dessa vez mais específicas ao escopo de mineração de padrões de alta utilidade:\nPerceba que, de acordo com essas definições, todos os algoritmos que são utilizados para mineração de padrões frequentes de alta utilidade podem também ser utilizados para minerar padrões frequentes, basta que a utilidade interna e externa de todos os itens seja definida com o mesmo valor, preferencialmente “1”. Para entender melhor as semelhanças e diferenças entre as duas técnicas de mineração de dados, verifique a tabela a seguir:"
  },
  {
    "objectID": "seminario1/artigo1.html#técnicas-e-algoritmos-usados",
    "href": "seminario1/artigo1.html#técnicas-e-algoritmos-usados",
    "title": "Artigo 1: A Survey of High Utility Itemset Mining",
    "section": "Técnicas e Algoritmos usados",
    "text": "Técnicas e Algoritmos usados\nO artigo estudado tem por objetivo apresentar a área de mineração de padrões frequentes de alta utilidade, além de mostrar ao leitor diferentes algoritmos para realizar essa mineração. As principais técnicas para a confecção dos algoritmos são a de “Duas fases” e de “Uma fase”, essas técnicas serão explicadas nas próximas subseções, juntamente com um algoritmo representante de cada classe.\n\nAlgoritmos de duas fases\nAlgoritmos que seguem essa técnica usam o conceito de “Utilidade da transação”, ou Transaction Utility (TU), que pode ser definido como a soma da utilidade de todos os itens que estão presentes em uma transação, para definir limites superiores do quão alta a utilidade de um subconjunto dessa transação pode ser. Esse limite superior é calculado para cada um dos padrões (itemsets) candidatos através de uma “Utilidade com peso em transações”, ou Transaction Weighted Utility (TWU), que é definida como a soma da utilidade de todas as transações que contêm o padrão em evidência.\nO valor obtido de TWU para um itemset, é o limite superior para todos os superconjuntos que possam ser formados a partir dele. Por exemplo, suponha a existência de um itemset base {a, b} que possui TWU igual a 10, isso significa que qualquer itemset da forma {a, b, _}, onde o terceiro e último item pode ser qualquer um do universo de itens disponívei, terá necessariamente uma utilidade menor que 10. A aplicação dessa propriedade nos algoritmos traz um importante avanço, que é o estabelecimento de um decrescimento monotônico do TWU de acordo com o aumento dos itens em um itemset.\nA partir dessas propriedades, é definido um suporte de utilidade mínimo que elimina todos os itemsets que tenham TWU abaixo desse limiar assim que são identificados, evitando a geração de superconjuntos que não têm chances de serem de alta utilidade.\n\n\n\nExemplo de cálculo do TWU.\n\n\nA primeira fase dos algoritmos consiste em gerar o TWU de todos os itens disponíveis, já que eles são o menor itemset possível (excluindo o conjunto vazio). Após o cálculo, todos os itemsets unitários que tenham um TWU que estejam abaixo do suporte de utilidade mínimo definido são eliminados do espaço de busca, evitando que candidatos infrutíferos sejam gerados. Em seguida, a geração de candidatos continua para os itemsets de dois elementos gerados a partir dos remanescentes do filtro anterior, sendo que esses novos candidatos serão também removidos no caso de terem TWU menor que o suporte mínimo. Essa sequência de ações continua em repetição até que já não seja mais possível gerar novos candidatos.\n\nNote que nessa primeira fase está sendo calculado o TWU, que é o limite superior de utilidade, e não a utilidade dos itemsets em si\n\nA segunda fase do algoritmo consiste em calcular a utilidade de todos os candidatos que sobraram da fase anterior, eliminando aqueles que tenham utilidade menor que o limite inferior estabelecido.\nO primeiro algoritmo desenvolvido para essa técnica se chama Two-Phase Algorithm, tendo sido baseado no algoritmo Apriori para mineração de padrões frequentes. É possível ver uma imagem do pseudocódigo desse algoritmo a seguir:\n\n\n\nPseudocódigo do algoritmo Two-Phase.\n\n\nPerceba que a função ITEMSETGENERATION() recebe apenas o conjunto de candidatos da iteração anterior como parâmetro, não verificando a base de dados de transação para gerar os candidatos, o que pode levar a itemsets que não ocorrem em nenhuma transação, resultando em um desperdício de tempo considerável para os cálculos deles.\nOutra limitação do algoritmo é que ele itera pelo conjunto de dados várias vezes para calcular o TWU dos itemsets, elevando o custo do algoritmo. Note que a exploração do espaço de busca desse algoritmo segue a técnica de Breadth First Search (BFS), o que leva a uma maior demora para eliminação de candidatos infrutíferos, principalmente pelo fato de que o TWU é uma métrica de limite extrapolada.\n\n\nAlgoritmos de uma fase\nEsses algoritmos são mais diretos, fazendo o cálculo da utilidade de cada padrão considerado no espaço de busca, o que permite identificar imediatamente se um itemset é de alta ou baixa utilidade sem a necessidade de guardá-lo em memória principal (RAM).\nOutra novidade desses algoritmos é que eles trazem uma nova forma de calcular os limites superiores de utilidade, sendo mais próxima à utilidade real dos itemsets do que o TWU usado nos algoritmos de duas fases. Como exemplo de algoritmo dessa técnica, será utilizado o Fast High-Utility Miner (FHM), que introduz o conceito de Listas de Utilidade, ou Utility List (UL), para representar o banco de dados das transações.\nConsiderando um itemset X, a lista de utilidade UL(X) será uma lista de tuplas para todas as ocorrências de X nas transações do banco, sendo que cada tupla armazenará o ID da transação em que o itemset está presente, a utilidade do itemset naquela transação e a soma da utilidade de todos os itens com ordem lexicográfica superior aos itens de X. O algoritmo se inicia calculando as listas de utilidade de todos os itemsets de um único elemento, sendo que as listas de utilidades dos superconjuntos desses itemsets será calculada a partir dos componentes delas.\nPor exemplo, suponha as listas de utilidade dos conjuntos unitários UL({a}) e UL({d}), para gerar a lista de utilidade e calcular a utilidade do itemset {a, d}, será calculada a interseção das transações que estão em UL({a}) e UL({d}). A utilidade do novo itemset será simplesmente a soma das utilidades dos itemsets geradores, enquanto a utilidade dos itens com ordem lexicográfica superior será igual a presente no itemset gerador de maior ordem lexicográfica, no caso do exemplo, será o mesmo de {b}.\n\n\n\nExemplo de cálculo das listas de utilidade.\n\n\nO cálculo do limite superior para esse algoritmo é chamado de “Limite Superior por Utilidade Residual”, ou Remaining Utility Upper-Bound, é feito somando a utilidade de um item (iutil) com a utilidade dos itens residuais de ordem lexicográfica maior (rutil) para todas as transações presentes na lista de utilidade. Caso esse resultado final seja menor que a utilidade mínima definida, aquele itemset é eliminado do espaço de busca, evitando que novos candidatos sejam gerados. A figura a seguir mostra o pseudocódigo para o algoritmo FHM:\n\n\n\nPseudocódigo do algoritmo FHM.\n\n\nAlgoritmos baseados em listas de utilidade, como o FHM, são até duas ordens de magnitude mais rápidos que os algoritmos de duas fases. Porém, a geração de candidatos ainda é baseada em itemsets anteriores, sem verificar o banco de dados de transações, o que pode levar a candidatos inexistentes e aumento no custo total do algoritmo por gastar recursos verificando possibilidades desnecessárias.\nAlém disso, o custo de memória para o armazenamento das listas de utilidade de cada itemset verificado pode vir a ser preocupante. Outro ponto de atenção de algoritmos que seguem essa estratégia é o fato de que são feitas muitas comparações com listas de utilidade anteriores no processo de geração de candidatos, já que um candidato com k itens deverá fazer comparações com k-1 listas de utilidade anteriores."
  },
  {
    "objectID": "seminario1/artigo1.html#metodologia-do-artigo",
    "href": "seminario1/artigo1.html#metodologia-do-artigo",
    "title": "Artigo 1: A Survey of High Utility Itemset Mining",
    "section": "Metodologia do artigo",
    "text": "Metodologia do artigo\nO artigo adota uma abordagem metodológica baseada em Survey, delineando inicialmente o problema em questão e, em seguida, apresentando algoritmos destinados à sua resolução. Uma filtragem criteriosa de artigos relevantes no domínio da mineração de itemsets de alta utilidade foi realizada, seguida pela compilação e síntese dos algoritmos destacados, abordando suas estruturas e conceitos fundamentais.\nOs primeiros algoritmos abrangentes para identificar conjuntos de itens de alta utilidade operam em duas fases distintas: primeiro, geram-se candidatos que são subsequentemente avaliados quanto à sua utilidade efetiva. Esses algoritmos introduziram uma inovação crucial ao estabelecer uma medida monótona que serviria como limite superior para a utilidade dos conjuntos de itens. Uma dessas medidas pioneiras foi a TWU (Transaction-Weighted Utilization), a qual permitiu uma poda eficiente do espaço de busca. Em estágios posteriores, surgiram algoritmos de uma única fase, cujo propósito é economizar tempo ao integrar a geração e avaliação de candidatos em um único passo. Vale ressaltar que muitos desses algoritmos propostos representam generalizações de técnicas de mineração de conjuntos de itens frequentes estabelecidas, como o Two Phase (uma extensão do Apriori) e o UP-Growth (uma extensão do FP-Growth).\nDentre os algoritmos apresentados para a mineração de padrões frequentes de alta utilidade são destacados os seguintes:\n\n\n\n\n\n\n\n\n\n\nAlgoritmo\nTipo de Busca\nFases\nRepresentação dos Dados\nExtende\n\n\n\n\nTwo-Phase\nBusca em Largura\nDuas\nHorizontal\nApriori\n\n\nHUP-Growth\nBusca em Profundidade\nDuas\nHorizontal (Árvore de Prefixos)\nFP-Growth\n\n\nD2HUP\nBusca em Profundidade\nUma\nVertical (Hiperestrutura)\nH-Mine\n\n\nFHM\nBusca em Profundidade\nUma\nVertical (Listas de Utilidade)\nEclat\n\n\nEFIM\nBusca em Profundidade\nUma\nVertical (com fusões)\nLCM\n\n\n\nO artigo porém não se contém somente em discutir os algoritmos completos de mineração de padrões, mas também, reconhecendo a importância de representações com um nível maior de significado. É nesse ponto em que são apresentados os algoritmos que mineram representações concisas dos subconjuntos de alta utilidade:\n\n\n\n\n\n\n\n\n\n\nAlgoritmo\nPadrões\nFases\nRepresentação dos Dados\nExtende\n\n\n\n\nMinFHM\nMinUIs\nUma\nVertical (Listas de Utilidade)\nFHM\n\n\nCHUD\nCHUIs\nDuas\nVertical (Listas de Utilidade)\nDCI Closed\n\n\nEFIM-CLOSED​\nCHUIs\nUma\nHorizontal (com fusões)\nEFM\n\n\nGUIDE\nMHUIs One\nUma\nStream\nUpGrowth\n\n\n\nPor fim, são apresentados algoritmos que retornam apenas os K subconjuntos de alta utilidade mais frequentes no conjunto de transações:\n\n\n\n\n\n\n\n\n\n\nAlgoritmo\nTipo de Busca\nFases\nRepresentação dos Dados\nExtende\n\n\n\n\nTKU​\nBusca em Profundidade​\nDuas\nHorizontal (Árvore de Prefixos)\n​ UP-Growth​\n\n\nTKO​\nBusca em Profundidade​\nUma\nVertical (Listas de Utilidade)\nHUI-Miner​\n\n\nREPT​\nBusca em Profundidade​\nUma\nHorizontal (Árvore de Prefixos)​\nMU-Growth​\n\n\nkHMC​\nBusca em Profundidade​\nUma\nVertical (Listas de Utilidade)​\nFHM​"
  },
  {
    "objectID": "seminario1/artigo1.html#aplicações-e-desafios-éticos-e-sociais",
    "href": "seminario1/artigo1.html#aplicações-e-desafios-éticos-e-sociais",
    "title": "Artigo 1: A Survey of High Utility Itemset Mining",
    "section": "Aplicações e Desafios Éticos e Sociais",
    "text": "Aplicações e Desafios Éticos e Sociais\nHá uma vasta gama de problemas do mundo real que podem se beneficiar significativamente do uso de algoritmos de mineração de subconjuntos frequentes de alta utilidade. Entre eles:\n\nMercado de Varejo: Potencial para aumentar os lucros ao impulsionar as vendas de produtos mais rentáveis.\nMercado de Compra Conjunta: Oportunidade de melhorar a lucratividade ao associar produtos visando redução de impostos.\nSistema de Recomendação: Aprimoramento da capacidade de gerar lucro ao focar em produtos mais rentáveis.\nCross-Selling e Up-Selling: Estímulo para compras de produtos complementares e promoção de vendas casadas.\nTratamento de Saúde: Desenvolvimento de conjuntos de tratamentos visando maior eficiência.\nDetecção de Fraudes: Identificação de padrões pouco frequentes, porém altamente úteis, na detecção de fraudes. Uso da Internet:__ Análise do comportamento dos usuários para aprimorar a importância do site.\nTelecomunicações: Utilização na identificação de padrões de comunicação que resultam em maior lucratividade.\nMineração de Texto: Identificação de textos com elevado valor agregado.\n\nNo entanto, a implementação de algoritmos de mineração de alta utilidade suscita preocupações sociais e éticas que demandam uma atenção cuidadosa. Um ponto crucial é a ameaça à privacidade, uma vez que a identificação de indivíduos a partir de dados aparentemente anônimos pode comprometer a segurança dos mesmos. Ademais, há o risco de manipulação do mercado e do comportamento do consumidor, onde o conhecimento de padrões de consumo pode ser utilizado de maneira indevida para influenciar escolhas.\nOutra questão relevante é a elisão fiscal, na qual empresas utilizam o conhecimento de padrões de alta utilidade para minimizar suas obrigações fiscais de forma legal, mas questionável do ponto de vista ético. Esses desafios destacam a importância de regulamentações sólidas e transparência no uso de algoritmos de mineração de dados, garantindo que o impacto social e ético seja considerado em todas as etapas, desde a implementação até a operação dessas ferramentas avançadas."
  },
  {
    "objectID": "seminario1/artigo1.html#como-usar",
    "href": "seminario1/artigo1.html#como-usar",
    "title": "Artigo 1: A Survey of High Utility Itemset Mining",
    "section": "Como usar",
    "text": "Como usar\nNeste guia iremos ensinar o passo a passo para poder executar o SPMF, um software livre que tem implementado vários algoritmos de mineração de itemsets de alta qualidade.\nA execução do SPMF exige o JAVA versão mínima 1.8, aqui iremos mostrar a instalação tanto do JAVA quanto do SPMF para o Windows, o processo de instalação do programa deve ser o mesmo no Linux, já que o programa é baseado em JAVA, a diferença se dará na instalação do JAVA.\n\nInstalação\nInicialmente segui o guia de instalação do JAVA deste link, mas na hora de execução do SPMF o programa não funcionou e a solução foi reinstalar o JAVA de outra forma. Apenas a fim de documentar um possível erro que você encontre ao tentar executar o SPMF, fica aqui o vídeo do processo de instalação que não funcionou.\n\n\n\n\n\nInstalação da versão errada do JAVA\n\n\nA instalação do SPMF é simples e se encontra neste link. O vídeo a seguir mostra o processo inteiro:\n\n\n\n\n\nInstalação do software SPMF\n\n\nComo dito anteriormente, no final obtemos um erro do JAVA que é concertado pela re-instalação de uma versão atual neste link, tal processo é mostrado no vídeo a seguir:\n\n\n\n\n\nInstalação da versão mais recente do JAVA\n\n\n\n\nExecução\n\nArquivo de Entrada\nO SPMF suporta arquivos de entrada no formato .txt.\nA primeira parte do arquivo de entrada é opcional e é usada para nomear os itens presentes no banco de dados.\n\nLinhas começando com @​.\nPrimeira linha com “@CONVERTED_FROM_TEXT”​\nDemais linhas fazem a ligação do item com sua descrição no formato @ITEM={ID}={DESCRICAO}\n\n{ID} é o número do item\n{DESCRICAO} é o nome do item\n\n\n\n\n\nPrimira parte do arquivo de entrada (opcional)\n\n\nA segunda parte contém os dados das transações, com cada linha representando uma transação e cada coluna separada por “:” contendo o itemset, a utilidade total do itemset e a utilidade de cada item do itemset, respectivamente.\n\nLinhas representam as transações​\nCada linha possui 3 colunas separadas pelo caractere ‘:’​\n\nColuna 1: itemset com os ids dos itens separados por espaço simples.\nColuna 2: utilidade total do itemset.\nColuna 3: utilidade respectiva de cada item do itemset separadas por espaço simples.​\n\n\n\n\n\nSegunda parte do arquivo de entrada\n\n\n\n\nExecução do Software\nVídeo tutorial de como se deve executar o programa a partir do dado de entrada, o algoritmo escolhido no tutorial foi o Two-Phase:\n\n\n\n\n\nApesar de no tutorial ser mostrado a execução com o algoritmo Two-Phase, temos várias opções de algoritmos para mineração de itensets de alta utilidade, como UP-Growth, UP-Growth+, FHM e HUI-Miner.\n\n\nArquivo de Saída\nA saída do algoritmo também é um arquivo .txt, contendo os itemsets de alta utilidade encontrados, o suporte do itemset (nem todos os algoritmos geram esse valor) e a utilidade do itemset.\n\nLinhas representam itemsets de alta utilidade encontrados.​\nCada linha possui 3 seções:​\n\nO itemset, com os ids ou nomes dos itens separados por espaço simples, depende da existência da 1ª parte do arquivo de entrada.​\n“#SUP: {VALOR}” onde {VALOR} é o suporte do itemset (nem todos os algoritmos geram esse valor)​\n“#UTIL: {VALOR}” onde {VALOR} é a utilidade do itemset.\n\n\n\n\n\nArquivo de saída"
  },
  {
    "objectID": "seminario1/artigo1.html#referências",
    "href": "seminario1/artigo1.html#referências",
    "title": "Artigo 1: A Survey of High Utility Itemset Mining",
    "section": "Referências",
    "text": "Referências\n\nFournier-Viger, P., Chun-Wei Lin, J., Truong-Chi, T., Nkambou, R. (2019). A Survey of High Utility Itemset Mining. In: Fournier-Viger, P., Lin, JW., Nkambou, R., Vo, B., Tseng, V. (eds) High-Utility Pattern Mining. Studies in Big Data, vol 51. Springer, Cham. https://doi.org/10.1007/978-3-030-04921-8_1\nFOURNIER-VIGER, P. et al. FHM: Faster High-Utility Itemset Mining Using Estimated Utility Co-occurrence Pruning. ReserachGate, Taiwan, jul./2014. Disponível em: https://www.researchgate.net/publication/263696687. Acesso em: 23 abr. 2024.\n\nFournier-Viger, Philippe. “SPMF: A Java Open-Source Pattern Mining Library.” Disponível em: https://www.philippe-fournier-viger.com/spmf/index.php. Acesso em: 22 de Abril de 2024.\nLIU, Mengchi; QU, Junfeng. Mining High Utility Itemsets without Candidate Generation. ResearchGate, Wuhan, nov./2020. Disponível em: https://www.researchgate.net/publication/262369808. Acesso em: 23 abr. 2024."
  },
  {
    "objectID": "seminario2.html",
    "href": "seminario2.html",
    "title": "Introdução",
    "section": "",
    "text": "Os seminários da disciplina consistem em uma apresentação coletiva (da turma) de um artigo mais recente sobre tópicos diretamente relacionados ao conteúdo visto em sala. A ideia é que a turma como um todo estude os artigos mais recentes da área e discuta esses trabalhos em sala. Em cada sessão, discutimos três artigos recentes relacionados aos tópicos abordados nas aulas teóricas.\nPara isso, adotamos um formato adaptado da proposta apresentada pelos Profs. Alec Jacobson e Colin Raffel, ambos da Universidade de Toronto (Canadá) – veja a proposta original em https://colinraffel.com/blog/role-playing-seminar.html. A proposta consiste em fazer uma encenação de papéis (role play) científicos para a apresentação do seminário. Nessa proposta, cada grupo cumprirá um papel na apresentação. Ao final, uma apresentação em formato de slides e um documento textual são produzidos. A apresentação é usada em sala de aula para fomentar as discussões, enquanto o documento fornece uma descrição textual das impressões da turma com a intenção de descrever o tema do artigo para um público amplo interessado em aprendizado de máquina e mineração de dados.\nSão apresentados a seguir os artigos discutidos no semestre 2024/1, com os respectivos links para os slides e documentos textuais apresentando os artigos.\n\n\nAnytime discovery of a diverse set of patterns with Monte Carlo tree search\nby Guillaume Bosc, Jean-François Boulicaut, Chedy Raissi, Tin Truong-Chi, and Mehdi Kaytoue. 2017\nhttps://doi.org/10.1007/s10618-017-0547-5"
  },
  {
    "objectID": "seminario2.html#artigo-4",
    "href": "seminario2.html#artigo-4",
    "title": "Introdução",
    "section": "",
    "text": "Anytime discovery of a diverse set of patterns with Monte Carlo tree search\nby Guillaume Bosc, Jean-François Boulicaut, Chedy Raissi, Tin Truong-Chi, and Mehdi Kaytoue. 2017\nhttps://doi.org/10.1007/s10618-017-0547-5"
  },
  {
    "objectID": "seminario2/artigo5.html",
    "href": "seminario2/artigo5.html",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "",
    "text": "A descoberta de subgrupos (SD) é um método de mineração de dados que visa identificar padrões interessantes dentro de grandes conjuntos de dados, destacando segmentos específicos que diferem de forma significativa do restante. Este método é particularmente útil para explorar dados e encontrar subgrupos que apresentam comportamentos ou características notáveis.\nNo contexto da SD, os dados numéricos desempenham um papel crucial. Eles podem ser usados tanto como atributos de descrição, que ajudam a definir os subgrupos, quanto como alvos, onde se analisa o valor numérico em si. A forma como esses dados numéricos são tratados pode impactar significativamente a qualidade e a utilidade dos subgrupos descobertos. Com isso, é importante que a aplicação de métodos SD sejam adaptadas para lidar com esse tipo de dado da melhor forma possível."
  },
  {
    "objectID": "seminario2/artigo5.html#contextualização-do-problema",
    "href": "seminario2/artigo5.html#contextualização-do-problema",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Contextualização do problema",
    "text": "Contextualização do problema\nHistoricamente, o tratamento de dados numéricos em algoritmos de descoberta de subgrupos tem sido predominantemente estático e global. Um exemplo disso é o mergeSD (2009), que abordou o tratamento de dados numéricos, embora com um aumento significativo no custo computacional. No entanto, muitos algoritmos clássicos ainda não tratam adequadamente os atributos numéricos, resultando na perda de informações relevantes e na diminuição da eficiência do modelo.\nPara abordar essa questão, o artigo investiga qual abordagem é mais eficaz para lidar com esses atributos, utilizando um framework que compara diferentes estratégias de discretização de dados numéricos e de busca de subgrupos. A motivação central dos autores é compreender como a discretização dos valores numéricos pode ser realizada de maneira a preservar a qualidade e a redundância mínima nos subgrupos descobertos. Assim, o artigo pode ser visto como uma revisão dos principais métodos de tratamento de atributos numéricos em SD."
  },
  {
    "objectID": "seminario2/artigo5.html#conceitos-importantes",
    "href": "seminario2/artigo5.html#conceitos-importantes",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Conceitos importantes",
    "text": "Conceitos importantes\nA descoberta de subgrupos lida com atributos numéricos por meio de condições nos valores, discretizando-os em intervalos para tornar o problema tratável. A discretização pode ser realizada de forma global, no início do processo, com todos os pontos de corte definidos de uma vez, ou de forma local, sendo realizada dinamicamente durante a mineração.\nOs intervalos gerados pela discretização podem ser binários ou nominais. Intervalos binários referem-se a divisões simples em duas categorias, como “baixo” e “alto”, enquanto intervalos nominais podem ter múltiplas categorias como “baixo”, “médio” e “alto”. No entanto, os autores mencionam que essa simplificação resultante da discretização pode descaracterizar os dados, pois os valores perdem certas propriedades ao serem agrupados em categorias. Por exemplo, ao comparar valores numéricos em um conjunto que varia de 1 a 10, os valores 7 e 8 podem ser categorizados como “alto”, fazendo com que 7 deixe de ser menor que 8, pois ambos são apenas “alto”.\nA tabela abaixo mostra um exemplo prático de discretização do atributo “height” (altura) de um grupo de 13 indivíduos usando a estratégia nominal e binária. Na estratégia nominal (coluna heightn), os pontos de corte foram definidos para os intervalos a ≤ 170 &lt; b ≤ 179 &lt; c. Já na estratégia binária temos duas categorias: low (coluna heightl) e low/medium (coluna heightlm) que correspondem a uma forma alternativa de discretizar os pontos de corte da estratégia nominal.\n\n\n\nExemplo de estratégias de discretização para atributos numéricos\n\n\nA granularidade para discretização refere-se ao nível de detalhe ou ao número de candidatos (intervalos) em que os dados numéricos são divididos. Granularidade fina envolve a criação de muitos intervalos pequenos, enquanto a granularidade grossa gera poucos intervalos maiores. Uma granularidade mais fina permite uma análise mais detalhada, mas pode aumentar o custo computacional. A seleção de candidatos pode incluir todos os possíveis subgrupos (all) ou apenas um subconjunto (best), o que também impacta o custo computacional."
  },
  {
    "objectID": "seminario2/artigo5.html#entendendo-os-algoritmos-usados",
    "href": "seminario2/artigo5.html#entendendo-os-algoritmos-usados",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Entendendo os algoritmos usados",
    "text": "Entendendo os algoritmos usados\nPara dar suporte à investigação, os pesquisadores parametrizaram e executaram dois algoritmos de base: um para a descoberta de subgrupos e outro para a discretização de atributos numéricos.\nO primeiro algoritmo, SDMM, realiza a busca de subgrupos válidos em uma base de dados, utilizando uma função de qualidade, restrições e estratégias de busca pré-estabelecidas. O algoritmo explora o dados, criando um primeiro subgrupo de busca e refinando o banco de dados a partir dele, com o objetivo de gerar novos candidatos para validação. Cada candidato é analisado, uma função de qualidade é associada a ele e é verificado se os candidatos seguem o padrão de qualidade estabelecido. Se sim, esses dados são adicionados ao conjunto solução; caso contrário, a busca é reiniciada.\nO segundo algoritmo, Equal Frequency Discretisation, é um algoritmo de discretização que desenvolve intervalos de classe. No início do processo, é definido como os atributos numéricos serão discretizados, estabelecendo o número de intervalos de classes e, a partir desse número, o algoritmo define os pontos de corte das classes para categorizar os atributos."
  },
  {
    "objectID": "seminario2/artigo5.html#estratégias-de-busca",
    "href": "seminario2/artigo5.html#estratégias-de-busca",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Estratégias de busca",
    "text": "Estratégias de busca\nPara gerar os quadros de simulações possíveis e comparar as melhores soluções de descoberta de subgrupos, são utilizados três métodos principais: busca em feixe tradicional, busca em feixe CBSS e busca completa.\nA busca em feixe tradicional é uma busca em nível, que limita o número de candidatos e a cada processamento. O método é ajustado para buscar subgrupos de acordo com um valor predefinido, restringindo o número de candidatos considerados em cada etapa.\nA busca em feixe em CBSS é uma variação do feixe tradicional, que gera supercandidatos preliminares. Para otimizar a busca dos melhores candidatos, ele incrementa o número de candidatos, visando aumentar a diversidade do processo.\nA busca completa realiza uma busca extensa usando todos os candidatos na busca de subgrupos. Embora isso aumente a qualidade da descoberta, há um custo operacional elevado e redundância. Trabalhar com grandes bancos de dados pode ser difícil e, apesar de melhorar a qualidade, a performance pode ser prejudicada.\n\n\n\nDiagrama de etapas do processamento de dados para descoberta de subgrupos"
  },
  {
    "objectID": "seminario2/artigo5.html#metodologia",
    "href": "seminario2/artigo5.html#metodologia",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Metodologia",
    "text": "Metodologia\nNo que diz respeito à metodologia do artigo, os autores realizaram uma série de testes, com todas as combinações de hiperparâmetros e estratégias possíveis. Ao todo, forma avaliados 5 aspectos diferentes de SD e como cada um deles impacta os resultados, são estes:\n\nHiperparâmetros, que se dividem em 2 tipos:\n\nNúmero de bins: quantidade de quebras na discretização.\nProfundidade: número de descritores.\n\nEstratégias para o SD numérico que variam em 4 dimensões, havendo duas opções para cada e, portanto, 2^4 possibilidades:\n\nMomento de discretização (local ou global).\nTipo de intervalo (binário ou nominal).\nGranularidade (fina ou grossa).\nMétodos de seleção (“all” ou “best”).\n\nDatasets utilizados, que podem ser para dois tipos de problemas:\n\nDatasets de classificação e regras não supervisionados, ex: covertype, credit-a, outros.\nDatasets de regressão, ex: auto-mpg, abalone, outros.\n\nEstratégia de busca:\n\nComplete search: envolve a exploração de todo o espaço de busca.\nBeam search: limita o número de candidatos a cada nível da busca.\nBeam search + CBSS: otimização para tentar reduzir a redundância dos subgrupos.\n\nMétricas de avaliação:\n\nWRAcc: para classificação.\nz-score: para regressão.\nEntropia conjunta: para calcular a redundância dos top 10 subgrupos.\n\n\nPara avaliar esses aspectos, foram realizados 17.020 experimentos, utilizando o algoritmo SDMM. Os experimentos utilizaram seis datasets de classificação e seis datasets de regressão e foram avaliados através de WRAcc para classificação e |z-score| para regressão."
  },
  {
    "objectID": "seminario2/artigo5.html#resultados-obtidos",
    "href": "seminario2/artigo5.html#resultados-obtidos",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Resultados obtidos",
    "text": "Resultados obtidos\nAs conclusões sobre vários aspectos obtidas com os resultados dos experimentos podem ser vistas de forma resumina no quadro a seguir:\n\n\n\n\n\n\n\nAspecto metodológico\nConclusão\n\n\n\n\nDiscretização\nNão existe regra universal para o número de bins. Estratégias nominais têm melhor desempenho com números de bins menores, mas estratégias binárias são preferíveis em todos os contextos e aplicações. A discretização local é sempre melhor.\n\n\nBusca\nA escolha da heurística de busca tem pouco impacto. Beam search é recomendada por ser eficiente e apresentar resultados bons quanto à complete search. Em datasets pequenos, a busca completa pode ser útil. CBSS reduz a redundância, mas não melhora a qualidade.\n\n\nMétricas de Qualidade\nAs métricas impactam significativamente nos subgrupos gerados, especialmente nos tamanhos, favorecendo subgrupos maiores. Avaliação feita com a correlação de Pearson.\n\n\nGranularidade\nA granularidade fina é melhor, mas em grandes profundidades, granularidades fina e grossa são equivalentes.\n\n\nMétodo de Seleção\nO método “all” é melhor, mas o “best” melhora a eficiência sem grandes perdas de qualidade.\n\n\n\nNo artigo, as estratégias utilizadas são denotadas por siglas que combinam várias letras, cada uma representando um hiperparâmetro específico da metodologia aplicada. Abaixo segue uma explicação simplificada do que cada letra nas iniciais das estratégias representa:\n\nL: Refere-se ao uso de discretização local.\nG: Refere-se ao uso de discretização global.\nB: Indica o uso de intervalos binários.\nN: Indica o uso de intervalos nominais.\nF: Representa a abordagem de granulidade fina na discretização dos dados.\nC: Representa a abordagem de granulidade grossa na discretização dos dados.\nA: Refere-se à categoria All no método de seleção.\nB: Refere-se à categoria Best no método de seleção.\n\nAssim, por exemplo, uma estratégia LBFA aplica discretização local com intervalos binários, granularidade fina e método de seleção “all”.\nPor fim, os autores apresentaram uma tabela dos resultados com as melhores escolhas para as 4 dimensões de estratégias em forma de ranking. Essa tabela se divide em duas partes, a primeira coluna que é baseada apenas na avaliação do melhor subgrupo gerado e a segunda leva em conta os 10 melhores subgrupos.\n\n\n\nRank final das estratégias utilizadas\n\n\nDe modo geral, os melhores resultados foram obtidos pelas estratégias LBFA e LBFB. Entretanto, esse ranking geral não tem garantia de estatística, visto que ele foi criado a partir de uma combinação de vários testes.\nVale ressaltar que uma estratégia em particular (LXFB) também apresentou resultados bons mas se distingue das demais por usar intervalos cartesianos como técnica de discretização, ao invés dos binários ou nominais. Devido às suas limitações de aplicabilidade, ela foi testada separadamente e teve performance melhor que todas as demais, sendo a melhor estratégia quando o objetivo principal é obter subgrupos de alta qualidade para classificação. No entanto, ela não é aplicável para regressão."
  },
  {
    "objectID": "seminario2/artigo5.html#aplicações-e-desafios",
    "href": "seminario2/artigo5.html#aplicações-e-desafios",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Aplicações e desafios",
    "text": "Aplicações e desafios\nA descoberta de subgrupos com atributos numéricos tem diversas aplicações, como por exemplo nas áreas da saúde, educação e setor financeiro. No entanto, também apresenta desafios específicos em cada uma dessas áreas.\nNa saúde, os modelos podem ser utilizados para predição de riscos, identificando subgrupos de pessoas mais suscetíveis a doenças de alto risco. Além disso, são úteis na análise de resposta a tratamentos, permitindo encontrar subgrupos que reagem de forma diferenciada a determinadas terapias, e no controle de doenças crônicas, identificando subgrupos que necessitam de cuidados específicos. Outra aplicação interessante é o mapeamento de áreas de risco em uma cidade, identificando regiões negligenciadas pelo sistema de saúde, que necessitem de maior atenção e investimento.\nNa educação, os modelos de descoberta de subgrupos podem ser aplicados para melhorar a performance acadêmica, identificando subgrupos de alunos com características similares para personalizar a abordagem pedagógica. Outra aplicação potencial é a predição de desistência, onde é possível identificar subgrupos de alunos com maiores índices de evasão, e assim, oferecer medidas de apoio adequadas. Adicionalmente, é possível identificar áreas de risco educacional, destacando regiões com baixo investimento em educação para criação de novas políticas públicas que visem melhorar a qualidade do ensino nessas áreas.\nNo setor financeiro, os modelos podem ser utilizados para otimização de portfólio, identificando subgrupos de ações que melhor se encaixam no perfil de determinados investidores. Além disso, também podem ser empregados na detecção de fraude, identificando subgrupos de transações que apresentam características anômalas. Na análise de crédito, é possível identificar subgrupos de pessoas com maior propensão a cometer fraudes bancárias, a fim de adotar medidas preventivas.\nApesar dos benefícios das diversas aplicações, a descoberta de subgrupos com atributos numéricos apresenta desafios comuns nas três áreas discutidas. A privacidade dos dados é uma preocupação constante, sendo necessário garantir o controle sobre onde os dados, como serão utilizados e quem pode acessá-los. A discriminação e a equidade de tratamento também são questões importantes, uma vez que a identificação de subgrupos pode gerar preferência de tratamento para determinados grupos, prejudicando outros."
  },
  {
    "objectID": "seminario2/artigo5.html#execução-dos-algoritmos-usados-no-artigo",
    "href": "seminario2/artigo5.html#execução-dos-algoritmos-usados-no-artigo",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Execução dos algoritmos usados no artigo",
    "text": "Execução dos algoritmos usados no artigo\nDois repositórios podem ser destacados para a execução do código proposto para análise de algoritmos de descoberta de subgrupos. O primeiro repositório é o da Universidade de Leiden, que disponibiliza o sistema Cortana. Esse sistema possui tanto o código-fonte quanto o binário compilado para execução. A página oficial do Cortana fornece informações gerais sobre a ferramenta e o binário pode ser baixado diretamente da página disponibilizada. Além disso, o código-fonte está disponível para desenvolvedores que desejam explorar ou modificar o sistema.\nO Cortana se destaca pela sua documentação extensa, encontrada no diretório /javadoc do código-fonte. No entanto, apresenta uma limitação significativa: a falta de um histórico de atualizações e uma descrição detalhada das funções. Isso pode dificultar a compreensão completa do sistema e a rastreabilidade de mudanças ao longo do tempo, o que é crucial para desenvolvedores e pesquisadores que precisam entender a evolução do software.\nO segundo repositório discutido foi o SubDisc, disponível no GitHub e atualizado regularmente, com a última atualização registrada em 12 de outubro de 2023. O SubDisc é escrito em Java e se foca principalmente na usabilidade e interface do usuário. Uma das grandes vantagens desse repositório é a inclusão de um guia detalhado em PDF, que fornece instruções claras sobre como utilizar a ferramenta, tornando-a acessível mesmo para aqueles que não são especialistas na área.\nA qualidade dos repositórios varia, mas ambos têm suas particularidades e utilidades. O Cortana, apesar de sua documentação extensa, carece de detalhes históricos e descrições de funções, o que pode ser um desafio. Por outro lado, o SubDisc se sobressai com sua documentação voltada para o usuário final e um guia de uso detalhado, embora não haja menção à profundidade da documentação técnica. A escolha entre essas ferramentas dependerá das necessidades específicas dos usuários, seja para um entendimento profundo do funcionamento interno ou para uma interface de usuário amigável.\nUm teste foi realizado utilizando a ferramenta Cortana. O dataset utilizado, tanto para regressão, quanto para classificação, foi o Adult, do repositório de Machine Learning da UCI. Na tarefa de classificação, o foco foi buscar subgrupos que apresentam um comportamento diferente da população em relação à variável binária “renda anual ≥ 50k”. Na tarefa de regressão, o foco está na idade dos indivíduos, verificando se há subgrupos com uma distribuição de idade atípica em relação à população geral.\nPara instalar o Cortana, baixe o arquivo cortana1782.jar e execute-o com o comando java abaixo na linha de comando.\njava -jar cortana1782.jar\nApós iniciar o programa, selecione o arquivo de dados nos formatos arff ou csv, ajuste os parâmetros conforme o tipo de objetivo (por exemplo, Single Numeric ou Single Nominal) e as medidas de qualidade desejadas (como Lift ou Z-Score). Verifique se os dados foram identificados corretamente e ajuste os tipos de atributos, desabilitando colunas indesejadas. Configure o método de descoberta, definindo parâmetros como profundidade de refinamento e cobertura mínima e máxima, e utilize a estratégia de beam search com um tamanho de beam de 100. Clique no botão “Subgroup Discovery” para iniciar o processo.\n\n\n\nInicialização do programa e carga de dados\n\n\nNo primeiro exemplo, para um problema de classificação onde se deseja identificar pessoas com renda anual ≥ 50k, foi configurado o método como Beam Search, com largura do beam de 100, profundidade de refinamento de 2, métrica de qualidade Lift (≥ 1.0) e cobertura entre 10% e 90%. Após selecionar o arquivo adult.csv e ajustar os parâmetros e atributos, a execução do algoritmo resultou em subgrupos como “education-num &gt; 12.0 AND capital-gain &gt; 5000”, com uma AUC de 0.85 na curva ROC.\n\n\n\nSubgrupos resultantes classificação\n\n\nNo segundo exemplo do problema de regressão, para identificar distribuições não usuais de idade, a configuração também incluiu Beam Search, com largura do beam de 100, profundidade de refinamento de 2 e cobertura entre 10% e 90%. Todavia, a métrica de qualidade foi Z-Score (≥ 2.0). A execução encontrou subgrupos como “relationship = husband AND hours-per-week &gt; 40”, permitindo a comparação da distribuição de idade com a população geral.\n\n\n\nSubgrupos resultantes regressão"
  },
  {
    "objectID": "seminario2/artigo5.html#conclusão",
    "href": "seminario2/artigo5.html#conclusão",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Conclusão",
    "text": "Conclusão\nO trabalho cumpre um papel importate na realização de experimentos capazes de confirmar algumas intuições não validadas até então e aprimorar o conhecimento em relações a estratégias numéricas. Nesse sentido, é um estudo bastante útil tanto em uma perspectiva aplicada, podendo servir como apoio na decisão de aspectos em aplicações reais, quanto no contexto de pesquisas e desenvolvimento de algoritmos."
  },
  {
    "objectID": "seminario2/artigo5.html#referências",
    "href": "seminario2/artigo5.html#referências",
    "title": "Artigo 5: For Real: A Thorough Look at Numeric Attributes in Subgroup Discovery",
    "section": "Referências",
    "text": "Referências\nBoley M, Goldsmith BR, Ghiringhelli LM, Vreeken J (2017) Identifying consistent statements about numerical data with dispersion-corrected subgroup discovery. Data Min Knowl Discov 31(5):1391–1418. https://doi.org/10.1007/s10618-017-0520-3."
  },
  {
    "objectID": "seminario3/artigo7.html",
    "href": "seminario3/artigo7.html",
    "title": "Artigo 7 - Mineração Supervisionada de Padrões Sequenciais em Esportes para Identificar Padrões de Jogo Importantes: Uma Aplicação ao Rugby Union",
    "section": "",
    "text": "A análise de desempenho esportivo tem se beneficiado cada vez mais da mineração de dados para extrair insights valiosos a partir de grandes volumes de informações. Em esportes de equipe, como o rugby union, entender padrões de jogo é crucial para melhorar estratégias e otimizar o desempenho dos atletas. Este artigo explora a aplicação da mineração supervisionada de padrões sequenciais para identificar sequências de eventos que influenciam diretamente os resultados em partidas de rugby."
  },
  {
    "objectID": "seminario3/artigo7.html#sobre-a-importância-dos-padrões-sequenciais",
    "href": "seminario3/artigo7.html#sobre-a-importância-dos-padrões-sequenciais",
    "title": "Artigo 7 - Mineração Supervisionada de Padrões Sequenciais em Esportes para Identificar Padrões de Jogo Importantes: Uma Aplicação ao Rugby Union",
    "section": "Sobre a importância dos padrões sequenciais",
    "text": "Sobre a importância dos padrões sequenciais\nPadrões sequenciais referem-se a sequências ordenadas de eventos que ocorrem ao longo do tempo, onde a ordem dos eventos é crucial para determinar um resultado específico. Em contextos esportivos, identificar tais padrões pode revelar insights sobre estratégias eficazes, vulnerabilidades adversárias e fatores determinantes para o sucesso durante uma partida.\n\n\n\nIlustrando padrões sequenciais\n\n\nNo rugby union, como em muitos outros esportes de equipe, os eventos são interdependentes e seu impacto no resultado do jogo depende muito da sequência em que ocorrem. Por exemplo, um passe bem-sucedido seguido de uma corrida eficaz pode levar a uma situação de pontuação, enquanto um passe mal executado pode resultar em uma perda de posse de bola. Assim, a análise de padrões sequenciais oferece uma visão mais aprofundada e contextualizada dos eventos de jogo, permitindo aos analistas esportivos entenderem melhor as dinâmicas de jogo que levam ao sucesso ou ao fracasso.\nAo contrário de abordagens tradicionais que podem se concentrar apenas na contagem de eventos isolados, a mineração de padrões sequenciais leva em consideração a interdependência entre eventos e sua influência cumulativa no resultado final de uma partida. Por exemplo, um único passe bem-sucedido pode não ser tão significativo quanto a sequência de eventos que leva a uma jogada de try."
  },
  {
    "objectID": "seminario3/artigo7.html#motivação-e-objetivos",
    "href": "seminario3/artigo7.html#motivação-e-objetivos",
    "title": "Artigo 7 - Mineração Supervisionada de Padrões Sequenciais em Esportes para Identificar Padrões de Jogo Importantes: Uma Aplicação ao Rugby Union",
    "section": "Motivação e Objetivos",
    "text": "Motivação e Objetivos\nO desenvolvimento de técnicas avançadas de análise de padrões sequenciais visa superar limitações das abordagens tradicionais, que geralmente se concentram apenas na frequência de eventos isolados. Neste estudo, aplicamos o algoritmo de Mineração de Padrões Sequenciais Supervisionada (Supervised Sequential Pattern Mining - SSPM) ao rugby union, com o objetivo de identificar padrões de jogo que discriminam entre situações de pontuação e não pontuação.\nOs principais objetivos deste estudo são:\n\nIdentificar padrões de eventos que precedem situações de pontuação no rugby union.\nComparar esses padrões com sequências que levam a eventos não pontuáveis.\nAvaliar a eficácia desses padrões em prever resultados de jogos.\nFornecer recomendações práticas para treinadores e analistas de desempenho com base nos padrões identificados."
  },
  {
    "objectID": "seminario3/artigo7.html#o-que-a-literatura-fala-sobre-isso",
    "href": "seminario3/artigo7.html#o-que-a-literatura-fala-sobre-isso",
    "title": "Artigo 7 - Mineração Supervisionada de Padrões Sequenciais em Esportes para Identificar Padrões de Jogo Importantes: Uma Aplicação ao Rugby Union",
    "section": "O que a literatura fala sobre isso",
    "text": "O que a literatura fala sobre isso\nA literatura já existente destaca várias abordagens para a mineração de padrões sequenciais em contextos esportivos. Métodos como Generalized Sequential Patterns (GSP), PrefixSpan e Spade têm sido amplamente utilizados para descobrir padrões frequentes em dados sequenciais. No entanto, essas técnicas geralmente não consideram explicitamente o impacto de padrões específicos na determinação de resultados positivos ou negativos em esportes de equipe.\nA abordagem de mineração de padrões sequenciais supervisionada (SSPM) difere das abordagens tradicionais ao incorporar rótulos de classe nos dados de treinamento, permitindo a identificação de padrões que são discriminativos para diferentes classes de resultado. Por exemplo, num estudo sobre futebol, [Bertens et al. (2016)] usaram mineração supervisionada para identificar padrões de passes que precedem situações de gol, enquanto [Sato e Sasaki (2018)] aplicaram técnicas semelhantes ao basquete para analisar sequências de jogadas que levam a cestas."
  },
  {
    "objectID": "seminario3/artigo7.html#como-essas-análises-foram-feitas",
    "href": "seminario3/artigo7.html#como-essas-análises-foram-feitas",
    "title": "Artigo 7 - Mineração Supervisionada de Padrões Sequenciais em Esportes para Identificar Padrões de Jogo Importantes: Uma Aplicação ao Rugby Union",
    "section": "Como essas análises foram feitas",
    "text": "Como essas análises foram feitas\n\nColeta de dados\nOs dados foram coletados a partir de partidas de rugby union utilizando sistemas de análise de desempenho que capturam eventos como passes, chutes, tackles e tries. Cada evento foi registrado temporalmente, formando sequências de eventos ao longo de cada partida.\nEsses dados foram obtidos através de plataformas de análise de desempenho, como Opta Sports e Stats Perform, que fornecem informações detalhadas sobre cada ação ocorrida durante as partidas. Além disso, gravações de vídeo das partidas foram analisadas para garantir a precisão dos dados coletados.\nEntão, os eventos foram coletados na seguinte estrutura:\n\n\n\n\n\n\n\n\nID do Evento\nEvento\nDescrição do Evento\n\n\n\n\n1\nRestart Received\nTime recebe um reinício de jogo com um chute feito pela equipe adversária\n\n\n2\nPhase\nPeríodo entre quebras (time em posse da bola)\n\n\n3\nBreakdown\nJogador do time é tackleado, resultando em um ruck\n\n\n4\nKick in Play\nChute dentro do campo de jogo (em vez de para fora) feito pelo time\n\n\n5\nPenalty Conceded\nTime concede uma penalidade, a equipe adversária pode recuperar a posse\n\n\n6\nKick at Goal\nTime tenta um chute ao gol\n\n\n\n\n\nPreparação dos dados\nOs dados brutos foram pré-processados para extrair sequências de eventos relevantes para a análise. Cada sequência foi rotulada com base no resultado final do evento (pontuação ou não pontuação), preparando assim o conjunto de dados para aplicação do SSPM. O pré-processamento incluiu a limpeza dos dados para remover inconsistências, a normalização temporal dos eventos e a segmentação das partidas em sequências de jogadas significativas, por exemplo:\n\n\nAplicação do SSPM\nO algoritmo SSPM foi aplicado aos dados preparados para identificar padrões sequenciais que são discriminativos em relação aos resultados de pontuação e não pontuação. O processo envolveu a descoberta de padrões que ocorrem com frequência significativa em cenários de pontuação, mas são menos frequentes em cenários de não pontuação, e vice-versa.\n\nAlgoritmo SSPM\nO algoritmo de mineração de padrões sequenciais supervisionada usado neste estudo baseia-se na extensão dos métodos tradicionais de mineração de padrões frequentes para incorporar rótulos de classe. Isso permite a identificação de padrões que não são apenas frequentes, mas também discriminativos para diferentes resultados de jogo.\nDe forma simplificada, o SSPM segue os seguintes passos:\n\nConstrução do Conjunto de Dados: As sequências de eventos são extraídas dos dados brutos e rotuladas com base no resultado (pontuação ou não pontuação).\nGeração de Padrões Candidatos: Sequências candidatas são geradas a partir do conjunto de dados, considerando diferentes combinações de eventos.\nAvaliação de Padrões: Os padrões candidatos são avaliados com base em métricas como suporte, confiança e lift, para determinar sua relevância e capacidade de discriminação entre classes.\nFiltragem de Padrões: Padrões que não atendem a critérios mínimos de suporte e confiança são descartados, enquanto os padrões significativos são retidos para análise posterior.\n\n\n\n\nAnálise\nOs padrões identificados pelo SSPM foram avaliados quanto à sua capacidade de discriminar entre diferentes resultados de jogo. Métricas como suporte (frequência do padrão no conjunto de dados), confiança (proporção de sequências contendo o padrão que resultam na classe de interesse) e lift (medida da importância relativa do padrão) foram utilizadas para medir a relevância e a significância estatística dos padrões descobertos.\nOs padrões foram então validados utilizando um conjunto de dados de teste separado, garantindo que os resultados não fossem enviesados pelo overfitting. A análise incluiu a visualização dos padrões identificados em contextos reais de jogo, proporcionando uma compreensão prática de como esses padrões influenciam os resultados."
  },
  {
    "objectID": "seminario3/artigo7.html#resultados",
    "href": "seminario3/artigo7.html#resultados",
    "title": "Artigo 7 - Mineração Supervisionada de Padrões Sequenciais em Esportes para Identificar Padrões de Jogo Importantes: Uma Aplicação ao Rugby Union",
    "section": "Resultados",
    "text": "Resultados\nO estudo identificou um conjunto de padrões sequenciais que demonstraram forte capacidade de discriminação entre situações de pontuação e não pontuação no rugby union. Entre os padrões mais significativos estavam sequências de eventos que precedem tries e conversões bem-sucedidas, assim como padrões defensivos que antecedem turnovers e penalidades concedidas.\n\nPadrões relevantes\nOs padrões mais relevantes incluíram combinações específicas de eventos como passes rápidos seguidos de corridas eficazes, além de formações defensivas que resultaram em turnovers estratégicos. Esses padrões não apenas refletiram estratégias eficazes de jogo, mas também indicaram áreas de melhoria potencial para equipes analisarem e treinarem.\nExemplo do formato de resultados:\n\n\n\n\n\n\n\nSequência de Eventos\nResultado\n\n\n\n\nRestart Received → Phase → Breakdown → Try Scored\nPontuação\n\n\nPhase → Breakdown → Kick in Play → Penalty Conceded\nNão Pontuação\n\n\nLine-out → Phase → Breakdown → Error\nNão Pontuação\n\n\nScrum → Phase → Line Breaks → Try Scored\nPontuação\n\n\nPenalty Conceded → O-Restart Received → O-Phase → O-Try Scored\nNão Pontuação\n\n\nError → O-Line-out → O-Phase → O-Kick at Goal\nNão Pontuação\n\n\n\n\nExemplo de padrão ofensivo\nUm dos padrões ofensivos mais significativos identificados foi a sequência: passe rápido → corrida curta → passe longo → try. Este padrão foi observado frequentemente em situações de pontuação e mostrou uma combinação eficaz de movimentação rápida e precisa da bola, seguida por uma corrida curta para atrair a defesa adversária, culminando em um passe longo que explorava espaços abertos para finalizar com um try.\n\n\nExemplo de padrão defensivo\nUm padrão defensivo relevante identificado foi a sequência: tackle eficaz → turnover → chute para fora. Este padrão refletiu uma estratégia defensiva eficaz onde um tackle bem executado resultava em perda de posse pelo adversário, seguido por um turnover estratégico e um chute para reposicionar o jogo, ganhando território e aliviando a pressão defensiva."
  },
  {
    "objectID": "seminario3/artigo7.html#considerações",
    "href": "seminario3/artigo7.html#considerações",
    "title": "Artigo 7 - Mineração Supervisionada de Padrões Sequenciais em Esportes para Identificar Padrões de Jogo Importantes: Uma Aplicação ao Rugby Union",
    "section": "Considerações",
    "text": "Considerações\nA aplicação bem-sucedida da SSPM ao rugby union destacou a importância de considerar a sequencialidade de eventos em análises de desempenho esportivo. Ao contrário de abordagens tradicionais baseadas apenas em estatísticas simples, como posse de bola ou número de chutes, a mineração de padrões sequenciais oferece uma perspectiva mais granular e orientada a eventos que são diretamente relevantes para o resultado final do jogo.\nOs padrões identificados têm implicações diretas para o desenvolvimento de estratégias de jogo. Por exemplo, o reconhecimento de padrões ofensivos eficazes pode informar a preparação de jogadas específicas em treinos, enquanto a identificação de padrões defensivos pode ajudar as equipes a desenvolver estratégias para interceptar e neutralizar ataques adversários.\n\nLimitações observadas\nEmbora os resultados sejam promissores, este estudo também enfrentou algumas limitações. A qualidade e a granularidade dos dados de eventos são cruciais para a precisão dos padrões identificados. Além disso, foi observado uma generalização dos padrões descobertos para diferentes níveis de competição (amador vs. profissional) e constatado que diferentes contextos de jogo (condições climáticas, local da partida) requerem uma análise mais aprofundada.\nOutro desafio é o potencial de overfitting, onde os padrões identificados podem ser específicos do conjunto de dados de treinamento e não se generalizam bem para novos dados. Isso foi mitigado através do uso de um conjunto de dados de teste separado, mas permanece uma consideração importante para futuras pesquisas.\n\n\nSobre o futuro\nEste estudo abre várias direções para futuras pesquisas em análise de desempenho esportivo. Uma área promissora é a aplicação de técnicas de aprendizado de máquina para prever resultados de jogos com base nos padrões sequenciais identificados. Além disso, a incorporação de dados biométricos e físicos dos jogadores pode enriquecer ainda mais a análise, fornecendo insights sobre como o condicionamento físico e a saúde dos atletas influenciam a execução de padrões de jogo.\n\n\nAplicações práticas\nAlém do rugby union, a metodologia SSPM pode ser aplicada a outros esportes de equipe, como futebol, basquete e hóquei, para melhorar a análise de desempenho, informar estratégias táticas e até mesmo otimizar o condicionamento físico dos atletas. As técnicas desenvolvidas neste estudo têm o potencial de transformar a forma como os treinadores e analistas esportivos abordam a preparação e o planejamento de partidas."
  },
  {
    "objectID": "seminario3/artigo7.html#conclusão",
    "href": "seminario3/artigo7.html#conclusão",
    "title": "Artigo 7 - Mineração Supervisionada de Padrões Sequenciais em Esportes para Identificar Padrões de Jogo Importantes: Uma Aplicação ao Rugby Union",
    "section": "Conclusão",
    "text": "Conclusão\nEm suma, a mineração supervisionada de padrões sequenciais aparece como uma ferramenta poderosa para analisar e entender o rugby union, oferecendo insights valiosos que podem influenciar diretamente a estratégia e o desempenho das equipes. Este estudo não apenas demonstra a viabilidade da SSPM em contextos do rugby, mas também destaca seu potencial para inovações futuras na análise de desempenho esportivo.\nA continuidade desta linha de pesquisa pode levar a avanços significativos na análise de desempenho esportivo, melhorando a tomada de decisão estratégica, otimizando o treinamento de atletas e elevando o nível competitivo em esportes de equipe ao redor do mundo."
  },
  {
    "objectID": "seminario3/artigo7.html#referências",
    "href": "seminario3/artigo7.html#referências",
    "title": "Artigo 7 - Mineração Supervisionada de Padrões Sequenciais em Esportes para Identificar Padrões de Jogo Importantes: Uma Aplicação ao Rugby Union",
    "section": "Referências",
    "text": "Referências\n\nBunker R, Fujii K, Hanada H, Takeuchi I (2021) Supervised sequential pattern mining of event sequences in sport to identify important patterns of play: An application to rugby union. PLOS ONE 16(9): e0256329. https://doi.org/10.1371/journal.pone.0256329\nBertens C, Elzinga D, van der Werf E (2016) Discovering sequential patterns in soccer matches to characterize defensive team strategies. PLOS ONE 11(10): e0165039. https://doi.org/10.1371/journal.pone.0165039\nSato K, Sasaki H (2018) Supervised learning for pattern discovery in basketball: A comparative evaluation of alternative approaches. Expert Systems with Applications 94: 280-290. https://doi.org/10.1016/j.eswa.2017.10.011\nAgrawal R, Srikant R (1995) Mining sequential patterns. In: Proceedings of the Eleventh International Conference on Data Engineering. IEEE, pp 3-14. https://doi.org/10.1109/ICDE.1995.380415\nSrikant R, Agrawal R (1996) Mining sequential patterns: Generalizations and performance improvements. In: Proceedings of the 5th International Conference on Extending Database Technology: Advances in Database Technology. Springer, pp 3-17. https://doi.org/10.1007/BFb0014140"
  }
]